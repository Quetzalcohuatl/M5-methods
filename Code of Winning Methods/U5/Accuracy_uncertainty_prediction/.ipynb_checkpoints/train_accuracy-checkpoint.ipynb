{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRALY & FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T11:12:39.973647Z",
     "start_time": "2020-07-08T11:12:38.179742Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##################### PREPARE\n",
    "####################################################################\n",
    "\n",
    "##################### Import\n",
    "####################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import shutil\n",
    "import gc\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import pickle\n",
    "from sympy import *\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import csr_matrix\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import sys,time, warnings, psutil, random\n",
    "from multiprocessing import Pool\n",
    "import decimal\n",
    "import holidays\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##################### some function\n",
    "####################################################################\n",
    "\n",
    "## シード選択\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "##################### save memory\n",
    "####################################################################\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "##################### データ作成\n",
    "####################################################################\n",
    "\n",
    "def make_df(path, features, VAL_START_DATE):\n",
    "    for i, filename in enumerate(features):\n",
    "        row = pd.read_pickle(path + filename + \".pkl\")\n",
    "        if i == 0:\n",
    "            df = pd.DataFrame({filename:row})\n",
    "        else:\n",
    "            df[filename] = row\n",
    "            \n",
    "            \n",
    "##################### データ分割\n",
    "####################################################################\n",
    "\n",
    "def data_division(df,):\n",
    "    \n",
    "    train = df[(df['date'] >= TRAIN_START_DATE) & (df[\"date\"] <= TRAIN_END_DATE)]\n",
    "    val = df[(df['date'] >= VAL_START_DATE) & (df[\"date\"] <= VAL_END_DATE)]\n",
    "    evaluation = df[(df['date'] >= EVAL_START_DATE) & (df[\"date\"] <= EVAL_END_DATE)]\n",
    "    evaluation_id_date = evaluation\n",
    "    train = train.drop([\"id\", \"date\"], axis=1)\n",
    "    val = val.drop([\"id\", \"date\"], axis=1)\n",
    "    evaluation = evaluation.drop([\"id\", \"date\"], axis=1)\n",
    "    print(\"Data分割完了\")\n",
    "    \n",
    "    tr_y = train[\"sales\"]\n",
    "    tr_x = train.drop(\"sales\", axis=1)\n",
    "    val_y = val[\"sales\"]\n",
    "    val_x = val.drop(\"sales\", axis=1)\n",
    "    eval_x = evaluation_id_date\n",
    "    eval_x_id_date = evaluation_id_date.drop(\"sales\", axis=1)\n",
    "    print(\"学習データ作成完了\")\n",
    "    \n",
    "    return tr_x, tr_y, val_x, val_y, eval_x, eval_x_id_date\n",
    "\n",
    "##################### feature importance graph\n",
    "####################################################################\n",
    "\n",
    "def graph_feature_importance(df):\n",
    "    df = df.sort_values(\"importances\", ascending=True)\n",
    "    plt.figure(figsize=(16, 50))\n",
    "    plt.barh([i for i in range(len(df.index))], df[\"importances\"])\n",
    "    plt.yticks([i for i in range(len(df.index))], df.index)\n",
    "    plt.title(\"feature_importance\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "##################### decay_learning rate\n",
    "####################################################################\n",
    "\n",
    "def decay_learning_rate(current_iter):\n",
    "    if current_iter < 200:\n",
    "        lr = 0.03\n",
    "    elif current_iter < 300:\n",
    "        lr = 0.01\n",
    "#     elif current_iter < 500:\n",
    "#         lr = 0.01\n",
    "#     elif current_iter < 1000:\n",
    "#         lr = 0.008\n",
    "#     elif current_iter < 1500:\n",
    "#         lr = 0.03\n",
    "#     elif current_iter < 3000:\n",
    "#         lr = 0.02\n",
    "    else:\n",
    "        lr = 0.005\n",
    "    return lr\n",
    "\n",
    "########################### Helper to make dynamic rolling lags\n",
    "#################################################################################\n",
    "\n",
    "def make_lag(LAG_DAY):\n",
    "    base_eval = eval_x[['id','sales']]\n",
    "    col_name = 'sales_residual_diff_28_roll_365_shift_{:02d}'.format(LAG_DAY)\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll_mean(LAG_DAY,):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    base_eval = eval_x[[\"id\", \"sales\"]]\n",
    "    col_name = f\"sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean\"\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "def make_lag_roll_std(LAG_DAY,):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    base_eval = eval_x[[\"id\", \"sales\"]]\n",
    "    col_name = f\"sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_std\"\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(shift_day).rolling(roll_wind).std())\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "def make_lag_roll_max(LAG_DAY,):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    base_eval = eval_x[[\"id\", \"sales\"]]\n",
    "    col_name = f\"sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_max\"\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(shift_day).rolling(roll_wind).max())\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "def make_lag_roll_min(LAG_DAY,):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    base_eval = eval_x[[\"id\", \"sales\"]]\n",
    "    col_name = f\"sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_min\"\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(shift_day).rolling(roll_wind).min())\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "def make_lag_roll_skew(LAG_DAY,):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    base_eval = eval_x[[\"id\", \"sales\"]]\n",
    "    col_name = f\"sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_skew\"\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(shift_day).rolling(roll_wind).skew())\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "def make_lag_roll_kurt(LAG_DAY,):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    base_eval = eval_x[[\"id\", \"sales\"]]\n",
    "    col_name = f\"sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_kurt\"\n",
    "    base_eval[col_name] = base_eval.groupby(['id'])[\"sales\"].transform(lambda x: x.shift(shift_day).rolling(roll_wind).kurt())\n",
    "    return base_eval[[col_name]]\n",
    "\n",
    "##################### WRMSSE\n",
    "####################################################################\n",
    "\n",
    "##################### CUSTOM METRIC\n",
    "####################################################################\n",
    "\n",
    "def weight_calc(data,\n",
    "                product, \n",
    "                category=None,\n",
    "#                 sales_train_val\n",
    "               ):\n",
    "\n",
    "    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n",
    "    \n",
    "    sales_train_val = pd.read_csv('/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "    \n",
    "    if category is not None:\n",
    "        sales_train_val = sales_train_val[sales_train_val[f\"{category_name}_id\"]==category]\n",
    "    \n",
    "    d_name = ['d_' + str(i+1) for i in range(1941)]\n",
    "    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n",
    "    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n",
    "    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n",
    "    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))\n",
    "    \n",
    "\n",
    "    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)\n",
    "    \n",
    "    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))<1\n",
    "\n",
    "    sales_train_val = np.where(flag,np.nan,sales_train_val)\n",
    "\n",
    "    # denominator of RMSSE / RMSSEの分母\n",
    "    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1942-start_no)\n",
    "    \n",
    "    # calculate the sales amount for each item/level\n",
    "    df_tmp = data[(data['date'] > '2016-04-25') & (data['date'] <= '2016-05-22')]\n",
    "    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n",
    "    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n",
    "    df_tmp = df_tmp[product.id].values\n",
    "    \n",
    "    weight2 = weight_mat_csr * df_tmp \n",
    "\n",
    "    weight2 = weight2/np.sum(weight2)\n",
    "\n",
    "    del sales_train_val\n",
    "    gc.collect()\n",
    "    \n",
    "    return weight1, weight2\n",
    "\n",
    "def wrmsse(preds, data):\n",
    "#     DAYS_PRED = preds // \n",
    "    # this function is calculate for last 28 days to consider the non-zero demand period\n",
    "    DAYS_PRED = 28\n",
    "    # actual obserbed values / 正解ラベル\n",
    "    y_true = data.get_label()\n",
    "    \n",
    "    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    \n",
    "    y_true = y_true.astype(np.float32)\n",
    "    preds = preds.astype(np.float32)\n",
    "    \n",
    "    # number of columns\n",
    "    num_col = DAYS_PRED\n",
    "\n",
    "    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n",
    "#     pdb.set_trace()\n",
    "    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n",
    "    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n",
    "    \n",
    "          \n",
    "    train = weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n",
    "    \n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(\n",
    "                            train[:,:num_col] - train[:,num_col:])\n",
    "                    ,axis=1) / weight1) * weight2)\n",
    "    \n",
    "    return 'wrmsse', score, False\n",
    "\n",
    "def wrmsse_simple(preds, data):\n",
    "    \n",
    "    # actual obserbed values / 正解ラベル\n",
    "    y_true = data.get_label()\n",
    "    \n",
    "    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    # number of columns\n",
    "    num_col = DAYS_PRED\n",
    "    \n",
    "    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n",
    "    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n",
    "    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n",
    "          \n",
    "    train = np.c_[reshaped_preds, reshaped_true]\n",
    "    \n",
    "    weight2_2 = weight2[:NUM_ITEMS]\n",
    "    weight2_2 = weight2_2/np.sum(weight2_2)\n",
    "    \n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(\n",
    "                            train[:,:num_col] - train[:,num_col:])\n",
    "                        ,axis=1) /  weight1[:NUM_ITEMS])*weight2_2)\n",
    "    \n",
    "    return 'wrmsse', score, False\n",
    "\n",
    "\n",
    "##################### CUSTOM OBJECTIVE\n",
    "####################################################################\n",
    "\n",
    "def obj_wrmsse(preds, data):\n",
    "    \n",
    "    dt1 = datetime.strptime(TRAIN_START_DATE,'%Y-%m-%d')\n",
    "    dt2 = datetime.strptime(TRAIN_END_DATE,'%Y-%m-%d')\n",
    "    dt = dt2 - dt1\n",
    "    TRAIN_DAYS = dt.days + 1\n",
    "    \n",
    "    y_true = data.get_label()\n",
    "\n",
    "    y_true = y_true[-(NUM_ITEMS * TRAIN_DAYS):]\n",
    "    preds = preds[-(NUM_ITEMS * TRAIN_DAYS):]\n",
    "\n",
    "    reshaped_preds = preds.reshape(TRAIN_DAYS, NUM_ITEMS).T\n",
    "    reshaped_true = y_true.reshape(TRAIN_DAYS, NUM_ITEMS).T\n",
    "    \n",
    "    #ロス計算値\n",
    "    pred_fm = df_fm\n",
    "#     pred_fm.iloc[:, 13:] += (reshaped_preds - reshaped_true) / np.array(weight_df1[\"w_store_id_&_item_id\"]).reshape(-1, 1)\n",
    "#     * np.array(weight_df2[\"w_store_id_&_item_id\"]).reshape(-1, 1)\n",
    "#     \n",
    "    \n",
    "    #予測値と正解ラベル\n",
    "    reshaped_preds_df = df_fm\n",
    "    reshaped_true_df = df_fm\n",
    "    reshaped_preds_df.iloc[:, 13:] = reshaped_preds\n",
    "    reshaped_true_df.iloc[:, 13:] = reshaped_true\n",
    "    \n",
    "    #空箱\n",
    "#     ps = df_fm\n",
    "#     ts = df_fm\n",
    "    \n",
    "    for level in [\"LEVEL1\", \"LEVEL2\", \"LEVEL3\", \"LEVEL4\", \"LEVEL5\", \"LEVEL10\"]:\n",
    "#         print(\"Start_{}\".format(level))\n",
    "        days = [\"d_{}\".format(i+1) for i in range(TRAIN_DAYS)]\n",
    "        ps = np.array(reshaped_preds_df.groupby(LEVEL[level])[days].transform(np.sum))\n",
    "        ts = np.array(reshaped_true_df.groupby(LEVEL[level])[days].transform(np.sum))\n",
    "#         pdb.set_trace()\n",
    "        pred_fm.iloc[:, 13:] += (ps - ts) / np.array(weight_df1[\"w_{}\".format(LEVEL[level][0])]).reshape(-1, 1) * np.array(weight_df2[\"w_{}\".format(LEVEL[level][0])]).reshape(-1, 1)\n",
    "#     \n",
    "#         pdb.set_trace()\n",
    "#         ps = ps.reset_index()\n",
    "#         idxes = list(ps.index)\n",
    "#         pdb.set_trace()\n",
    "#         col = LEVEL[level][0]\n",
    "        \n",
    "        \n",
    "#         for idx in idxes:\n",
    "#             pred_fm.loc[pred_fm[col] == idx, days] += ps.loc[idx, days] - ts.loc[idx, days]\n",
    "#         print(\"Finish_{}\".format(level))\n",
    "    \n",
    "    y_p = np.array(pred_fm.iloc[:, 13:]).T.flatten()     \n",
    "\n",
    "    grad = -y_p/(TRAIN_DAYS*NUM_ITEMS)\n",
    "    hess = np.array([1/(TRAIN_DAYS*NUM_ITEMS) for i in range(TRAIN_DAYS*NUM_ITEMS)])\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "def obj_wrmsse2(preds, data):\n",
    "    y = data.get_label()\n",
    "    w = data.get_weight()\n",
    "    yhat = preds\n",
    "    grad = w*(yhat - y)\n",
    "    hess = np.ones_like(yhat)\n",
    "    return grad, hess\n",
    "\n",
    "def obj_wrmsse4(preds, data):\n",
    "    y = data.get_label()\n",
    "#     w = data.get_weight()\n",
    "    yhat = preds\n",
    "    diff = yhat - y\n",
    "    w = np.tile(WEIGHT_SCALED_30490, TRAIN_DAYS)\n",
    "    \n",
    "    diff = diff * w\n",
    "    \n",
    "    grad = diff\n",
    "    hess = w\n",
    "    \n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "##################### Dataset weights\n",
    "####################################################################\n",
    "\n",
    "def train_weight(df,):\n",
    "    lgb_w_dup = pd.read_pickle(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/weight/weight2_div_max.pkl\")\n",
    "    df = df[[\"id\", \"date\"]]\n",
    "    df = df.merge(lgb_w_dup, left_on=\"id\", right_on=\"id\")\n",
    "    lgb_w_tr = df.loc[(df[\"date\"] >= TRAIN_START_DATE) & (df[\"date\"] <= TRAIN_END_DATE), \"weight_total_div_max\"]\n",
    "    lgb_w_val = df.loc[(df[\"date\"] >= VAL_START_DATE) & (df[\"date\"] <= VAL_END_DATE), \"weight_total_div_max\"]\n",
    "    lgb_w_eval = df.loc[(df[\"date\"] >= EVAL_START_DATE) & (df[\"date\"] <= EVAL_END_DATE), \"weight_total_div_max\"]\n",
    "#     df = df.drop(\"weight\", axis=1)\n",
    "    return lgb_w_tr, lgb_w_val, lgb_w_eval\n",
    "\n",
    "\n",
    "##################### COST FORMAT\n",
    "####################################################################\n",
    "\n",
    "def cost_format():\n",
    "    sales_train_val = pd.read_csv('/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "    NUM_ITEMS = 30490\n",
    "    dt1 = datetime.strptime(TRAIN_START_DATE,'%Y-%m-%d')\n",
    "    dt2 = datetime.strptime(TRAIN_END_DATE,'%Y-%m-%d')\n",
    "    dt = dt2 - dt1\n",
    "    TRAIN_DAYS = dt.days + 1\n",
    "    df_fm = sales_train_val.iloc[:, 0:6+TRAIN_DAYS]\n",
    "    df_fm.iloc[:, 6:]=0\n",
    "    df_fm.insert(1, \"total_id\", [\"total\" for i in range(NUM_ITEMS)])\n",
    "    df_fm.insert(1, \"state_id_&_cat_id\", df_fm[\"state_id\"] + df_fm[\"cat_id\"])\n",
    "    df_fm.insert(1, \"state_id_&_dept_id\", df_fm[\"state_id\"] + df_fm[\"dept_id\"])\n",
    "    df_fm.insert(1, \"store_id_&_cat_id\", df_fm[\"store_id\"] + df_fm[\"cat_id\"])\n",
    "    df_fm.insert(1, \"store_id_&_dept_id\", df_fm[\"store_id\"] + df_fm[\"dept_id\"])\n",
    "    df_fm.insert(1, \"state_id_&_item_id\", df_fm[\"state_id\"] + df_fm[\"item_id\"])\n",
    "    df_fm.insert(1, \"store_id_&_item_id\", df_fm[\"store_id\"] + df_fm[\"item_id\"])\n",
    "    return df_fm\n",
    "\n",
    "##################### WEIGHT DATAFRAME\n",
    "####################################################################\n",
    "\n",
    "def w_df(df, weight_df):\n",
    "    weight_df = weight_df.iloc[:, 1:13]\n",
    "    weight_df[\"w_store_id_&_item_id\"] = df[12350:]\n",
    "    for level in [\"LEVEL{}\".format(i) for i in range(1, 12)]:\n",
    "#         days = [\"d_{}\".format(i) for i in range(1, 29)]\n",
    "        weight_df[\"w_{}\".format(LEVEL[level][0])] = weight_df.groupby(LEVEL[level])[\"w_store_id_&_item_id\"].transform(np.sum)\n",
    "    weight_df=weight_df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 12]]\n",
    "    weight_df_num = weight_df.iloc[:, 12:]\n",
    "    return weight_df_num, weight_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:24:14.056967Z",
     "start_time": "2020-07-06T16:24:14.052174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "########################### 固定変数\n",
    "#################################################################################\n",
    "\n",
    "seed_everything(seed=28)\n",
    "\n",
    "CAT_FEATURES = ['id_serial', 'cat_id', 'dept_id','event_name_1', 'item_id', \"wday\", \"day\", \n",
    "                'store_id','state_id', \"sell_price_minority12\", #\"year\", \"month\",\n",
    "                \"event_type_statecat_labelenc\", \"moon\"]# \"wday_day_labelenc\"] #\"day_y_snap_labelenc\", \"wday_snap_labelenc\", , #\"week_of_month\", \"week_of_year\"]\n",
    "\n",
    "CATEGORY_ID = [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\", \"TX_1\", \"TX_2\", \"TX_3\", \"WI_1\", \"WI_2\", \"WI_3\"]  ####\n",
    "category_name = \"store\"\n",
    "\n",
    "# SHIFT_DAY = 1\n",
    "\n",
    "# N_CORES = psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:13:13.375941Z",
     "start_time": "2020-07-08T06:13:13.157814Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "calendar_df = pd.read_csv('m5-forecasting-accuracy/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:13:20.045304Z",
     "start_time": "2020-07-08T06:13:20.023388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>Friday</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "      <td>Father's day</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  wm_yr_wk    weekday  wday  month  year       d  \\\n",
       "0     2011-01-29     11101   Saturday     1      1  2011     d_1   \n",
       "1     2011-01-30     11101     Sunday     2      1  2011     d_2   \n",
       "2     2011-01-31     11101     Monday     3      1  2011     d_3   \n",
       "3     2011-02-01     11101    Tuesday     4      2  2011     d_4   \n",
       "4     2011-02-02     11101  Wednesday     5      2  2011     d_5   \n",
       "...          ...       ...        ...   ...    ...   ...     ...   \n",
       "1964  2016-06-15     11620  Wednesday     5      6  2016  d_1965   \n",
       "1965  2016-06-16     11620   Thursday     6      6  2016  d_1966   \n",
       "1966  2016-06-17     11620     Friday     7      6  2016  d_1967   \n",
       "1967  2016-06-18     11621   Saturday     1      6  2016  d_1968   \n",
       "1968  2016-06-19     11621     Sunday     2      6  2016  d_1969   \n",
       "\n",
       "      event_name_1 event_type_1  event_name_2 event_type_2  snap_CA  snap_TX  \\\n",
       "0              NaN          NaN           NaN          NaN        0        0   \n",
       "1              NaN          NaN           NaN          NaN        0        0   \n",
       "2              NaN          NaN           NaN          NaN        0        0   \n",
       "3              NaN          NaN           NaN          NaN        1        1   \n",
       "4              NaN          NaN           NaN          NaN        1        0   \n",
       "...            ...          ...           ...          ...      ...      ...   \n",
       "1964           NaN          NaN           NaN          NaN        0        1   \n",
       "1965           NaN          NaN           NaN          NaN        0        0   \n",
       "1966           NaN          NaN           NaN          NaN        0        0   \n",
       "1967           NaN          NaN           NaN          NaN        0        0   \n",
       "1968  NBAFinalsEnd     Sporting  Father's day     Cultural        0        0   \n",
       "\n",
       "      snap_WI  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "...       ...  \n",
       "1964        1  \n",
       "1965        0  \n",
       "1966        0  \n",
       "1967        0  \n",
       "1968        0  \n",
       "\n",
       "[1969 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T09:23:09.889958Z",
     "start_time": "2020-07-04T09:23:03.428522Z"
    }
   },
   "outputs": [],
   "source": [
    "train_val_df = pd.read_csv('m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "sell_price_df = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('m5-forecasting-accuracy/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T09:54:40.483942Z",
     "start_time": "2020-07-04T09:54:29.519484Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sell_price_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-10edacf9338e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m###### price feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m##############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0msell_price_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msell_price_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"store_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0msell_price_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dept_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msell_price_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0msell_price_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msell_price_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dept_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sell_price_df' is not defined"
     ]
    }
   ],
   "source": [
    "###### Basic\n",
    "##############################\n",
    "\n",
    "### Basic Category ID\n",
    "day_list = []\n",
    "for i in range(1942, 1970):\n",
    "    day_list.append(\"d_{}\".format(i))\n",
    "for day in day_list:\n",
    "    train_val_df[day] = np.nan\n",
    "    \n",
    "train_val_df[\"id_serial\"] = list(range(30490))\n",
    "    \n",
    "melt_sales = pd.melt(train_val_df, id_vars=['id_serial', 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name=\"d\", value_name=\"sales\")\n",
    "\n",
    "\n",
    "####### Calendar feature\n",
    "#event2\n",
    "calendar_df.loc[calendar_df[\"event_name_2\"]== \"Cinco De Mayo\", \"event_name_2\"] = 0\n",
    "calendar_df.loc[calendar_df[\"event_name_2\"]== \"Easter\", \"event_name_2\"] = 1\n",
    "calendar_df.loc[calendar_df[\"event_name_2\"]== \"Father's day\", \"event_name_2\"] = 2\n",
    "calendar_df.loc[calendar_df[\"event_name_2\"]== \"OrthodoxEaster\", \"event_name_2\"] = 3\n",
    "calendar_df.loc[calendar_df[\"event_type_2\"]== \"Cultural\", \"event_type_2\"] = 0\n",
    "calendar_df.loc[calendar_df[\"event_type_2\"]== \"Religious\", \"event_type_2\"] = 1\n",
    "calendar_df[\"event_name_2\"] = calendar_df[\"event_name_2\"].astype(np.float16)\n",
    "calendar_df[\"event_type_2\"] = calendar_df[\"event_type_2\"].astype(np.float16)\n",
    "\n",
    "#weekend\n",
    "calendar_df[\"is_weekend\"] = 0\n",
    "calendar_df.loc[calendar_df[\"weekday\"] == \"Saturday\", \"is_weekend\"] = 1\n",
    "calendar_df.loc[calendar_df[\"weekday\"] == \"Sunday\", \"is_weekend\"] = 1\n",
    "\n",
    "# d_serial\n",
    "calendar_df[\"d_serial\"] = calendar_df[\"d\"].apply(lambda x: int(x[2:]))\n",
    "\n",
    "# w_serial\n",
    "calendar_df[\"w_serial\"] = 0\n",
    "cnt = 1\n",
    "for i in range(calendar_df.shape[0]):\n",
    "    if i % 7 == 0:\n",
    "        cnt += 1\n",
    "    calendar_df.loc[i, \"w_serial\"] = cnt\n",
    "\n",
    "# date\n",
    "calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%Y-%m-%d')\n",
    "\n",
    "# day\n",
    "calendar_df[\"day\"] = calendar_df[\"date\"].apply(lambda x: x.day)\n",
    "calendar_df[\"month\"] = calendar_df[\"date\"].apply(lambda x: x.month)\n",
    "calendar_df[\"year\"] = calendar_df[\"date\"].apply(lambda x: x.year)\n",
    "\n",
    "# Mooon\n",
    "dec = decimal.Decimal\n",
    "\n",
    "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "    diff = d - datetime(2001, 1, 1)\n",
    "    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
    "    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
    "    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
    "    return int(phase_index) % 8\n",
    "\n",
    "calendar_df['moon'] = calendar_df.date.apply(get_moon_phase)\n",
    "\n",
    "# week of month, year\n",
    "calendar_df[\"week_of_month\"] = calendar_df[\"w_serial\"] - calendar_df.groupby([\"year\", \"month\"])[\"w_serial\"].transform(\"min\") +1\n",
    "calendar_df[\"week_of_year\"] = calendar_df[\"w_serial\"] - calendar_df.groupby([\"year\"])[\"w_serial\"].transform(\"min\") +1\n",
    "\n",
    "\n",
    "# olympic_president_elec_year\n",
    "calendar_df[\"olympic_president_elec_year\"] = 0\n",
    "calendar_df.loc[calendar_df[\"year\"] == 2012, \"olympic_president_elec_year\"] = 1\n",
    "calendar_df.loc[calendar_df[\"year\"] == 2016, \"olympic_president_elec_year\"] = 1\n",
    "\n",
    "\n",
    "# NBA_Finals\n",
    "calendar_df[\"NBA_finals\"] = 0\n",
    "for day in [\"d_123\", \"d_124\", \"d_125\", \"d_126\", \"d_127\", \"d_128\", \"d_129\", \"d_130\", \"d_131\", \"d_132\", \"d_133\", \"d_134\", \"d_135\", \n",
    "          \"d_501\", \"d_502\", \"d_503\", \"d_504\", \"d_505\", \"d_506\", \"d_507\", \"d_508\", \"d_509\", \"d_510\", \n",
    "          \"d_860\", \"d_861\", \"d_862\", \"d_863\", \"d_864\", \"d_865\", \"d_866\", \"d_867\", \"d_868\", \"d_869\", \"d_870\", \"d_871\", \"d_872\", \"d_873\", \"d_874\", \n",
    "          \"d_1224\", \"d_1225\", \"d_1226\", \"d_1227\", \"d_1228\", \"d_1229\", \"d_1230\", \"d_1231\", \"d_1232\", \"d_1233\", \"d_1234\", \n",
    "          \"d_1588\", \"d_1589\", \"d_1590\", \"d_1591\", \"d_1592\", \"d_1593\", \"d_1594\", \"d_1595\", \"d_1596\", \"d_1597\", \"d_1598\", \"d_1599\", \"d_1600\", \n",
    "          \"d_1952\", \"d_1953\", \"d_1954\", \"d_1955\", \"d_1956\", \"d_1957\", \"d_1958\", \"d_1959\", \"d_1960\", \"d_1961\", \"d_1962\", \"d_1963\", \"d_1964\", \n",
    "          \"d_1965\", \"d_1966\", \"d_1967\", \"d_1968\", \"d_1969\"]:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day  , \"NBA_finals\"] = 1\n",
    "\n",
    "    \n",
    "# Ramadan start\n",
    "day_list = []\n",
    "for i in range(30):\n",
    "    day_list.append(\"d_{}\".format(i+185))\n",
    "    day_list.append(\"d_{}\".format(i+539))\n",
    "    day_list.append(\"d_{}\".format(i+893))\n",
    "    day_list.append(\"d_{}\".format(i+1248))\n",
    "    day_list.append(\"d_{}\".format(i+1602))\n",
    "for i in range(13):\n",
    "    day_list.append(\"d_{}\".format(i+1957))\n",
    "\n",
    "calendar_df[\"Ramadan_Starts\"] = 0\n",
    "for day in day_list:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"Ramadan_Starts\"] = 1\n",
    "    \n",
    "# Mothers day \n",
    "day_list_1 = [\"d_100\", \"d_471\", \"d_835\", \"d_1199\", \"d_1563\", \"d_1927\"]\n",
    "day_list_2 = [\"d_99\", \"d_470\", \"d_834\", \"d_1198\", \"d_1562\", \"d_1926\"]\n",
    "calendar_df[\"Mothers_day\"] = 0\n",
    "for day in day_list_1:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"Mothers_day\"] = 1\n",
    "for day in day_list_2:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"Mothers_day\"] = 2\n",
    "\n",
    "# OrthodoxEaster\n",
    "day_list_1 = [\"d_86\", \"d_443\", \"d_828\", \"d_1178\", \"d_1535\", \"d_1920\"]\n",
    "day_list_2 = [\"d_85\", \"d_442\", \"d_827\", \"d_1177\", \"d_1534\", \"d_1919\"]\n",
    "calendar_df[\"OrthodoxEaster\"] = 0\n",
    "for day in day_list_1:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"OrthodoxEaster\"] = 1\n",
    "for day in day_list_2:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"OrthodoxEaster\"] = 2\n",
    "\n",
    "# Easter\n",
    "day_list_1 = [\"d_86\", \"d_436\", \"d_793\", \"d_1178\", \"d_1528\", \"d_1885\"]\n",
    "day_list_2 = [\"d_85\", \"d_435\", \"d_792\", \"d_1177\", \"d_1527\", \"d_1884\"]\n",
    "calendar_df[\"Easter\"] = 0\n",
    "for day in day_list_1:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"Easter\"] = 1\n",
    "for day in day_list_2:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"Easter\"] = 2\n",
    "    \n",
    "\n",
    "# IndependenceDay\n",
    "day_list_1 = [\"d_151\", \"d_517\", \"d_882\", \"d_1247\", \"d_1612\"]\n",
    "day_list_2 = [\"d_152\", \"d_518\", \"d_883\", \"d_1248\", \"d_1613\"]\n",
    "day_list_3 = [\"d_153\", \"d_519\", \"d_884\", \"d_1249\", \"d_1614\"]\n",
    "day_list_4 = [\"d_154\", \"d_520\", \"d_885\", \"d_1250\", \"d_1615\"]\n",
    "day_list_5 = [\"d_155\", \"d_521\", \"d_886\", \"d_1251\", \"d_1616\"]\n",
    "day_list_6 = [\"d_156\", \"d_522\", \"d_887\", \"d_1252\", \"d_1617\"]\n",
    "day_list_7 = [\"d_157\", \"d_523\", \"d_888\", \"d_1253\", \"d_1618\"]\n",
    "day_list_8 = [\"d_158\", \"d_524\", \"d_889\", \"d_1254\", \"d_1619\"]\n",
    "day_list_9 = [\"d_159\", \"d_525\", \"d_890\", \"d_1255\", \"d_1620\"]\n",
    "day_list_10 = [\"d_160\", \"d_526\", \"d_891\", \"d_1256\", \"d_1621\"]\n",
    "\n",
    "calendar_df[\"IndependenceDay\"] = 0\n",
    "for day in day_list_1:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 1\n",
    "for day in day_list_2:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 2\n",
    "for day in day_list_3:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 3\n",
    "for day in day_list_4:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 4\n",
    "for day in day_list_5:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 5\n",
    "for day in day_list_6:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 6\n",
    "for day in day_list_7:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 7\n",
    "for day in day_list_8:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 8\n",
    "for day in day_list_9:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 9\n",
    "for day in day_list_10:\n",
    "    calendar_df.loc[calendar_df[\"d\"] == day, \"IndependenceDay\"] = 10\n",
    "\n",
    "\n",
    "\n",
    "###### price feature\n",
    "##############################\n",
    "sell_price_df[\"state_id\"] = sell_price_df[\"store_id\"].apply(lambda x: x[:2])\n",
    "sell_price_df[\"dept_id\"] = sell_price_df[\"item_id\"].apply(lambda x: x[:-4])\n",
    "sell_price_df[\"cat_id\"] = sell_price_df[\"dept_id\"].apply(lambda x: x[:-2])\n",
    "\n",
    "sell_price_df[\"price_unique_item_state\"] = sell_price_df.groupby(['state_id','item_id'])['sell_price'].transform('nunique')\n",
    "sell_price_df[\"price_unique_item_store\"] = sell_price_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
    "\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "sell_price_df = sell_price_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "\n",
    "sell_price_df['price_momentum_m_item_state'] = sell_price_df['sell_price'] / sell_price_df.groupby(['state_id','item_id','month'])['sell_price'].transform('mean')\n",
    "sell_price_df['price_momentum_y_item_state'] = sell_price_df['sell_price'] / sell_price_df.groupby(['state_id','item_id','year'])['sell_price'].transform('mean')\n",
    "sell_price_df['price_momentum_m_item_store'] = sell_price_df['sell_price'] / sell_price_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "sell_price_df['price_momentum_y_item_store'] = sell_price_df['sell_price'] / sell_price_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "\n",
    "# sell_start\n",
    "sell_price_df[\"one\"] = 1\n",
    "sell_price_df[\"sell_start\"] = sell_price_df.groupby([\"store_id\", \"item_id\"])[\"one\"].transform(lambda x: x.cumsum())\n",
    "sell_price_df[\"sell_start_log\"] = sell_price_df.groupby([\"store_id\", \"item_id\"])[\"sell_start\"].transform(lambda x: np.log(x))\n",
    "\n",
    "sell_price_df = sell_price_df.drop([\"month\", \"year\", \"one\", \"state_id\", \"dept_id\", \"cat_id\"], axis=1)\n",
    "\n",
    "\n",
    "####### merge\n",
    "melt_sales = melt_sales.merge(calendar_df, on=\"d\", how=\"left\")\n",
    "melt_sales = pd.merge(melt_sales, sell_price_df, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')\n",
    "\n",
    "####### save by column\n",
    "melt_sales = reduce_mem_usage(melt_sales)\n",
    "for col in melt_sales.columns:\n",
    "    melt_sales[col].to_pickle(f\"features/Basic/{col}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T09:27:56.600291Z",
     "start_time": "2020-07-04T09:27:54.515102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_val_df, calendar_df, sell_price_df, melt_sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T14:29:54.452247Z",
     "start_time": "2020-07-04T14:29:01.892095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1030.57 Mb (25.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "####### TARGET\n",
    "###############################\n",
    "ID = pd.read_pickle(\"features/Basic/id.pkl\")\n",
    "sales = pd.read_pickle(\"features/Basic/sales.pkl\")\n",
    "df = pd.DataFrame({\"id\": ID,\n",
    "                   \"sales\": sales})\n",
    "\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "df[\"shift_28_roll_365\"] = df.groupby([\"id\"])[\"sales\"].transform(lambda x: x.shift(28).rolling(365).mean())\n",
    "df[\"sales_residual_diff_28_roll_365\"] = df[\"sales\"] - df[\"shift_28_roll_365\"]\n",
    "\n",
    "df[\"shift_28_roll_365\"].to_pickle(\"features/Target/shift_28_roll_365.pkl\")\n",
    "df[\"sales_residual_diff_28_roll_365\"].to_pickle(\"features/Target/sales_residual_diff_28_roll_365.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T10:12:41.212364Z",
     "start_time": "2020-07-04T09:56:23.042448Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The following 'id_vars' are not present in the DataFrame: ['first_sales']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a69818e005e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m melt_sales = pd.melt(df, id_vars=['id', 'item_id',\n\u001b[1;32m    106\u001b[0m \u001b[0;31m#                                             'distribution',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                                             'dept_id', 'cat_id', 'store_id', 'state_id', \"last_sales\", \"first_sales\"], var_name=\"day\", value_name=\"sales\")\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mmelt_sales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"last_sales\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features/Basic/last_sales.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/melt.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;34m\"The following 'id_vars' are not present \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;34m\"in the DataFrame: {missing}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 )\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The following 'id_vars' are not present in the DataFrame: ['first_sales']\""
     ]
    }
   ],
   "source": [
    "####### snap total\n",
    "###############################\n",
    "state_id = pd.read_pickle(\"features/Basic/state_id.pkl\")\n",
    "snap_CA = pd.read_pickle(\"features/Basic/snap_CA.pkl\")\n",
    "snap_TX = pd.read_pickle(\"features/Basic/snap_TX.pkl\")\n",
    "snap_WI = pd.read_pickle(\"features/Basic/snap_WI.pkl\")\n",
    "df = pd.DataFrame({\"state_id\":state_id,\n",
    "                   \"snap_CA\":snap_CA,\n",
    "                   \"snap_TX\":snap_TX,\n",
    "                   \"snap_WI\":snap_WI})\n",
    "\n",
    "df[\"snap_total\"] = 0\n",
    "df.loc[df.state_id == \"CA\", \"snap_total\"] = df.loc[df.state_id == \"CA\", \"snap_CA\"]\n",
    "df.loc[df.state_id == \"TX\", \"snap_total\"] = df.loc[df.state_id == \"TX\", \"snap_TX\"]\n",
    "df.loc[df.state_id == \"WI\", \"snap_total\"] = df.loc[df.state_id == \"WI\", \"snap_WI\"]\n",
    "\n",
    "df[\"snap_total\"].to_pickle(\"features/Basic/snap_total.pkl\") \n",
    "\n",
    "\n",
    "####### event_type_statecat_labelenc\n",
    "###############################\n",
    "train_val_df = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "calendar_df = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "\n",
    "day_list = []\n",
    "for i in range(1914, 1970):\n",
    "    day_list.append(\"d_{}\".format(i))\n",
    "for day in day_list:\n",
    "    train_val_df[day] = np.nan\n",
    "    \n",
    "melt_sales = pd.melt(train_val_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name=\"day\", value_name=\"demand\")\n",
    "melt_sales = melt_sales.merge(calendar_df, left_on=\"day\", right_on=\"d\")\n",
    "melt_sales = melt_sales[[\"state_id\", \"cat_id\", \"event_type_1\"]]\n",
    "melt_sales[\"event_type_statecat\"] = np.nan\n",
    "\n",
    "df_evtyp = melt_sales.loc[(melt_sales.event_type_1 == 'Cultural') | \\\n",
    "               (melt_sales.event_type_1 == 'National') | \\\n",
    "               (melt_sales.event_type_1 == 'Religious') | \\\n",
    "               (melt_sales.event_type_1 == 'Sporting'), :]\n",
    "\n",
    "df_evtyp[\"event_type_statecat\"] = df_evtyp[\"state_id\"] + df_evtyp[\"cat_id\"] + df_evtyp[\"event_type_1\"]\n",
    "\n",
    "melt_sales.loc[(melt_sales.event_type_1 == 'Cultural') | \\\n",
    "               (melt_sales.event_type_1 == 'National') | \\\n",
    "               (melt_sales.event_type_1 == 'Religious') | \\\n",
    "               (melt_sales.event_type_1 == 'Sporting'), \"event_type_statecat\"] = df_evtyp[\"event_type_statecat\"].values\n",
    "\n",
    "melt_sales[\"event_type_statecat_labelenc\"] = 0\n",
    "\n",
    "type_list = ['CAHOBBIESSporting', 'CAHOUSEHOLDSporting', 'CAFOODSSporting',\n",
    "       'TXHOBBIESSporting', 'TXHOUSEHOLDSporting', 'TXFOODSSporting',\n",
    "       'WIHOBBIESSporting', 'WIHOUSEHOLDSporting', 'WIFOODSSporting',\n",
    "       'CAHOBBIESCultural', 'CAHOUSEHOLDCultural', 'CAFOODSCultural',\n",
    "       'TXHOBBIESCultural', 'TXHOUSEHOLDCultural', 'TXFOODSCultural',\n",
    "       'WIHOBBIESCultural', 'WIHOUSEHOLDCultural', 'WIFOODSCultural',\n",
    "       'CAHOBBIESNational', 'CAHOUSEHOLDNational', 'CAFOODSNational',\n",
    "       'TXHOBBIESNational', 'TXHOUSEHOLDNational', 'TXFOODSNational',\n",
    "       'WIHOBBIESNational', 'WIHOUSEHOLDNational', 'WIFOODSNational',\n",
    "       'CAHOBBIESReligious', 'CAHOUSEHOLDReligious', 'CAFOODSReligious',\n",
    "       'TXHOBBIESReligious', 'TXHOUSEHOLDReligious', 'TXFOODSReligious',\n",
    "       'WIHOBBIESReligious', 'WIHOUSEHOLDReligious', 'WIFOODSReligious']\n",
    "\n",
    "for num, ty in zip(range(1, 37), type_list):\n",
    "    melt_sales.loc[melt_sales[\"event_type_statecat\"] == ty, \"event_type_statecat_labelenc\"] = num\n",
    "    \n",
    "melt_sales[\"event_type_statecat_labelenc\"].to_pickle(\"features/Basic/event_type_statecat_labelenc.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "###### national holiday\n",
    "#############################\n",
    "date = pd.read_pickle(\"features/Basic/date.pkl\")\n",
    "state_id = pd.read_pickle(\"features/Basic/state_id.pkl\")\n",
    "df = pd.DataFrame({\"date\":date,\n",
    "                   \"state_id\":state_id})\n",
    "\n",
    "df[\"National_Holiday\"] = 0\n",
    "for state in [\"CA\", \"TX\", \"WI\"]:\n",
    "    for year in [2011, 2012, 2013, 2014, 2015, 2016]:\n",
    "        for d, name in sorted(holidays.US(state=state, years=year).items()):\n",
    "            df.loc[(df.state_id==state)&(df.date==d.strftime('%Y-%m-%d')), \"National_Holiday\"] = 1\n",
    "\n",
    "df[\"National_Holiday\"].to_pickle(\"features/Basic/national_holiday.pkl\")\n",
    "            \n",
    "    \n",
    "    \n",
    "###### Last Sales\n",
    "##############################\n",
    "df = pd.read_csv('m5-forecasting-accuracy/sales_train_validation.csv')\n",
    "df[\"last_sales\"] = 0\n",
    "\n",
    "day = 1913\n",
    "for i in range(1913):\n",
    "    df.loc[(df[\"d_{}\".format(day)]>0) & (df[\"last_sales\"] == 0), \"last_sales\"] = day\n",
    "    if np.sum(df[\"last_sales\"] == 0) == 0:\n",
    "        break\n",
    "    day -= 1\n",
    "\n",
    "day_list = []\n",
    "for i in range(1914, 1970):\n",
    "    day_list.append(\"d_{}\".format(i))\n",
    "for day in day_list:\n",
    "    df[day] = np.nan\n",
    "    \n",
    "melt_sales = pd.melt(df, id_vars=['id', 'item_id',\n",
    "#                                             'distribution', \n",
    "                                            'dept_id', 'cat_id', 'store_id', 'state_id', \"last_sales\",], var_name=\"day\", value_name=\"sales\")\n",
    "\n",
    "melt_sales[\"last_sales\"].to_pickle(\"features/Basic/last_sales.pkl\")\n",
    "\n",
    "\n",
    "### minority\n",
    "\n",
    "sell_price = pd.read_pickle(\"features/Basic/sell_price.pkl\")\n",
    "\n",
    "df = pd.DataFrame({\"sell_price\":sell_price})\n",
    "\n",
    "def minority(num):\n",
    "    f, i = math.modf(num)\n",
    "    f = str(f)\n",
    "    return f[2:4]\n",
    "\n",
    "df[\"sell_price_minority12\"] = df[\"sell_price\"].transform(lambda x: minority(x))\n",
    "df.loc[df[\"sell_price_minority12\"] == \"n\", \"sell_price_minority12\"] = 9999\n",
    "df[\"sell_price_minority12\"] = df[\"sell_price_minority12\"].apply(lambda x: int(x))\n",
    "\n",
    "df[\"sell_price_minority12\"].to_pickle(\"features/Basic/sell_price_minority12.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T10:47:13.457890Z",
     "start_time": "2020-07-04T10:21:45.240426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3664.23 Mb (28.9% reduction)\n",
      "LEVEL2\n",
      "LEVEL3\n",
      "LEVEL4\n",
      "LEVEL5\n",
      "LEVEL6\n",
      "LEVEL7\n",
      "LEVEL8\n",
      "LEVEL9\n",
      "LEVEL10\n",
      "LEVEL11\n",
      "LEVEL12\n",
      "LEVEL2\n",
      "LEVEL3\n",
      "LEVEL4\n",
      "LEVEL5\n",
      "LEVEL6\n",
      "LEVEL7\n",
      "LEVEL8\n",
      "LEVEL9\n",
      "LEVEL10\n",
      "LEVEL11\n",
      "LEVEL12\n",
      "LEVEL2\n",
      "LEVEL3\n",
      "LEVEL4\n",
      "LEVEL5\n",
      "LEVEL6\n",
      "LEVEL7\n",
      "LEVEL8\n",
      "LEVEL9\n",
      "LEVEL10\n",
      "LEVEL11\n",
      "LEVEL12\n",
      "LEVEL2\n",
      "LEVEL3\n",
      "LEVEL4\n",
      "LEVEL5\n",
      "LEVEL6\n",
      "LEVEL7\n",
      "LEVEL8\n",
      "LEVEL9\n",
      "LEVEL10\n",
      "LEVEL11\n",
      "LEVEL12\n"
     ]
    }
   ],
   "source": [
    "###### average encoding\n",
    "##############################\n",
    "\n",
    "ID = pd.read_pickle(\"features/Basic/id.pkl\")\n",
    "item_id = pd.read_pickle(\"features/Basic/item_id.pkl\")\n",
    "store_id = pd.read_pickle(\"features/Basic/store_id.pkl\")\n",
    "dept_id = pd.read_pickle(\"features/Basic/dept_id.pkl\")\n",
    "state_id = pd.read_pickle(\"features/Basic/state_id.pkl\")\n",
    "cat_id = pd.read_pickle(\"features/Basic/cat_id.pkl\")\n",
    "d_serial = pd.read_pickle(\"features/Basic/d_serial.pkl\")\n",
    "sell_price = pd.read_pickle(\"features/Basic/sell_price.pkl\")\n",
    "wday = pd.read_pickle(\"features/Basic/wday.pkl\")\n",
    "day = pd.read_pickle(\"features/Basic/day.pkl\")\n",
    "sales = pd.read_pickle(\"features/Target/sales_residual_diff_28_roll_365.pkl\")\n",
    "\n",
    "df = pd.DataFrame({\"id\":ID,\n",
    "                   \"item_id\":item_id,\n",
    "                   \"store_id\":store_id,\n",
    "                   \"dept_id\":dept_id,\n",
    "                   \"state_id\":state_id,\n",
    "                   \"cat_id\":cat_id,\n",
    "                   \"d_serial\":d_serial,\n",
    "                   \"sell_price\":sell_price,\n",
    "                   \"wday\":wday,\n",
    "                   \"day\":day,\n",
    "                   \"sales\":sales})\n",
    "\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "df[\"flag\"] = 0\n",
    "df.loc[df[\"sell_price\"]>=0, \"flag\"] = 1\n",
    "df.loc[df[\"flag\"]==0, \"sales\"] = np.nan\n",
    "\n",
    "pred_terms = [\"private\", \"public\", \"validation\", \"semival\"]\n",
    "\n",
    "for i, term in enumerate(pred_terms):\n",
    "    \n",
    "    df.loc[df[\"d_serial\"]>=1942-i*28, \"sales\"] = np.nan\n",
    "\n",
    "    LEVEL = {\n",
    "                 \"LEVEL2\": [\"state_id\"],\n",
    "                 \"LEVEL3\": [\"store_id\"],\n",
    "                 \"LEVEL4\": [\"cat_id\"],\n",
    "                 \"LEVEL5\": [\"dept_id\"],\n",
    "                 \"LEVEL6\": [\"state_id\", \"cat_id\"],\n",
    "                 \"LEVEL7\": [\"state_id\", \"dept_id\"],\n",
    "                 \"LEVEL8\": [\"store_id\", \"cat_id\"],\n",
    "                 \"LEVEL9\": [\"store_id\", \"dept_id\"],\n",
    "                 \"LEVEL10\": [\"item_id\"],\n",
    "                 \"LEVEL11\": [\"state_id\", \"item_id\"],\n",
    "                 \"LEVEL12\": [\"store_id\", \"item_id\"]}\n",
    "\n",
    "    for key, value in LEVEL.items():\n",
    "        df.groupby(value + [\"wday\"])[\"sales\"].transform(np.mean).to_pickle(f\"features/Encoding/{term}_sales_residual_diff_28_roll_365_enc_week_{key}_mean.pkl\")   #df[f\"enc_{key}_mean\"]\n",
    "        df.groupby(value + [\"wday\"])[\"sales\"].transform(np.std).to_pickle(f\"features/Encoding/{term}_sales_residual_diff_28_roll_365_enc_week_{key}_std.pkl\")    #df[f\"enc_{key}_std\"] \n",
    "        df.groupby(value + [\"day\"])[\"sales\"].transform(np.mean).to_pickle(f\"features/Encoding/{term}_sales_residual_diff_28_roll_365_enc_day_{key}_mean.pkl\")   #df[f\"enc_{key}_mean\"]\n",
    "        df.groupby(value + [\"day\"])[\"sales\"].transform(np.std).to_pickle(f\"features/Encoding/{term}_sales_residual_diff_28_roll_365_enc_day_{key}_std.pkl\")    #df[f\"enc_{key}_std\"] \n",
    "        df.groupby(value)[\"sales\"].transform(np.mean).to_pickle(f\"features/Encoding/{term}_sales_residual_diff_28_roll_365_enc_{key}_mean.pkl\")   #df[f\"enc_{key}_mean\"]\n",
    "        df.groupby(value)[\"sales\"].transform(np.std).to_pickle(f\"features/Encoding/{term}_sales_residual_diff_28_roll_365_enc_{key}_std.pkl\")    #df[f\"enc_{key}_std\"] \n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### LAG Rolling\n",
    "#############################\n",
    "\n",
    "ID = pd.read_pickle(\"features/Basic/id.pkl\")\n",
    "d_serial = pd.read_pickle(\"features/Basic/d_serial.pkl\")\n",
    "sales = pd.read_pickle(\"features/Target/sales_residual_diff_28_roll_365.pkl\")\n",
    "df = pd.DataFrame({\"id\":ID,\n",
    "                   \"d_serial\":d_serial,\n",
    "                   \"sales\":sales})\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "#### shift 1~ 56\n",
    "for shift_day in range(1, 57):\n",
    "    df.groupby([\"id\"])[\"sales\"].transform(lambda x: x.shift(shift_day)).to_pickle(f\"features/Lag_Features/sales_residual_diff_28_roll_365_shift_{shift_day}.pkl\")\n",
    "\n",
    "#### rolling\n",
    "####################\n",
    "for i in [1, 2, 3, 5, 7]:\n",
    "    df[f\"multi_{i}\"] = df[\"d_serial\"].transform(lambda x: x%i)\n",
    "\n",
    "for shift_day in range(1, 37):\n",
    "    target = f\"sales_residual_diff_28_roll_365_shift_{shift_day}\"\n",
    "    \n",
    "    df[target] = pd.read_pickle(f\"features/Lag_Features/sales_residual_diff_28_roll_365_shift_{shift_day}.pkl\")\n",
    "    #### multi 2, 3, 5 rolling\n",
    "    for roll_wind in [3, 6, 10]:\n",
    "        df.groupby(['id', 'multi_2'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_2_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "        df.groupby(['id', 'multi_3'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_3_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "        df.groupby(['id', 'multi_5'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_5_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "        \n",
    "\n",
    "    #### multi 7 rolling\n",
    "    for roll_wind in [2, 3, 4, 8, 12]:\n",
    "        if roll_wind in [4, 8]:\n",
    "            df.groupby(['id', 'multi_7'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_7_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "            df.groupby(['id', 'multi_7'])[target].transform(lambda x: x.rolling(roll_wind).max()).to_pickle(f\"features/Lag_Features/multi_7_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_max.pkl\")\n",
    "            df.groupby(['id', 'multi_7'])[target].transform(lambda x: x.rolling(roll_wind).min()).to_pickle(f\"features/Lag_Features/multi_7_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_min.pkl\")\n",
    "        else:\n",
    "            df.groupby(['id', 'multi_7'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_7_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "\n",
    "    #### multi 1 rolling\n",
    "    for roll_wind in [7, 14, 30, 60]: \n",
    "        if roll_wind in [7, 30]:\n",
    "            df.groupby(['id', 'multi_1'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_1_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "            df.groupby(['id', 'multi_1'])[target].transform(lambda x: x.rolling(roll_wind).std()).to_pickle(f\"features/Lag_Features/multi_1_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_std.pkl\")\n",
    "            df.groupby(['id', 'multi_1'])[target].transform(lambda x: x.rolling(roll_wind).max()).to_pickle(f\"features/Lag_Features/multi_1_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_max.pkl\")\n",
    "            df.groupby(['id', 'multi_1'])[target].transform(lambda x: x.rolling(roll_wind).min()).to_pickle(f\"features/Lag_Features/multi_1_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_min.pkl\")\n",
    "            df.groupby(['id', 'multi_1'])[target].transform(lambda x: x.rolling(roll_wind).median()).to_pickle(f\"features/Lag_Features/multi_1_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_median.pkl\")\n",
    "        else:\n",
    "            df.groupby(['id', 'multi_1'])[target].transform(lambda x: x.rolling(roll_wind).mean()).to_pickle(f\"features/Lag_Features/multi_1_sales_residual_diff_28_roll_365_shift_{shift_day}_roll_{roll_wind}_mean.pkl\")\n",
    "    \n",
    "    df = df.drop(target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight (Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T11:27:14.942185Z",
     "start_time": "2020-07-08T11:24:37.836644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1603.10 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n"
     ]
    }
   ],
   "source": [
    "### make WEIGHT_SCALED_42840, WEIGHT_FORMAT, WEIGHT_SCALED_30490\n",
    "\n",
    "train_val_df = pd.read_csv('m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "sell_price_df = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "\n",
    "day_list = []\n",
    "for i in range(1942, 1970):\n",
    "    day_list.append(\"d_{}\".format(i))\n",
    "for day in day_list:\n",
    "    train_val_df[day] = np.nan\n",
    "    \n",
    "melt_sales = pd.melt(train_val_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name=\"day\", value_name=\"sales\")\n",
    "calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%Y-%m-%d')\n",
    "melt_sales = melt_sales.merge(calendar_df, left_on=\"day\", right_on=\"d\")\n",
    "melt_sales = pd.merge(melt_sales, sell_price_df, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')\n",
    "\n",
    "melt_sales = melt_sales[[\"id\", \"date\", \"sales\", \"sell_price\"]]\n",
    "\n",
    "melt_sales = reduce_mem_usage(melt_sales)\n",
    "\n",
    "############## カテゴリと州ごとのデータフレーム作成 \n",
    "# train_val_df = train_val_df[train_val_df[\"distribution\"]==\"poisson\"]\n",
    "\n",
    "NUM_ITEMS = train_val_df.shape[0]\n",
    "\n",
    "product = train_val_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "# 重みマップ作成\n",
    "print(\"重みマップ作成・・・・\")\n",
    "weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n",
    "                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n",
    "                   ].T\n",
    "\n",
    "np.save(f\"weights/weight_mat_total.npy\" ,weight_mat)\n",
    "weight_mat_csr = csr_matrix(weight_mat)\n",
    "\n",
    "#     del train_val_df, sell_price_df, calendar_df, weight_mat\n",
    "#     gc.collect()\n",
    "\n",
    "############## ロス関数の重み算出、保存\n",
    "print(\"ロス関数重み計算・・・・\")\n",
    "weight1, weight2 = weight_calc(melt_sales, product,)\n",
    "# del df, train_val_df; gc.collect()\n",
    "#     del melt_sales; gc.collect()\n",
    "\n",
    "############## SAVE WEIGHT\n",
    "\n",
    "np.save(\"weights/weight1_total.npy\" ,weight1)\n",
    "np.save(\"weights/weight2_total.npy\" ,weight2)\n",
    "\n",
    "# train_val_df = pd.read_csv('/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "# product = train_val_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "\n",
    "df = pd.DataFrame(np.ones([30490,1]).astype(np.int8), index=product.index, columns=[\"total\"])\n",
    "df = pd.concat((df, pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8')), axis=1)\n",
    "df = pd.concat((df, pd.DataFrame(np.identity(30490).astype(np.int8), index=product.index, columns=product[\"id\"])), axis=1)\n",
    "\n",
    "df.index = product.id\n",
    "df = df.T\n",
    "\n",
    "df.to_pickle(\"weights/weight_size_change_format.pkl\")\n",
    "\n",
    "weight_scaled = weight2**2/weight1\n",
    "\n",
    "WEIGHT_SCALED_42840 = weight_scaled / weight_scaled[0]\n",
    "WEIGHT_SCALED_30490 = np.dot(WEIGHT_SCALED_42840, df.values)\n",
    "\n",
    "np.save(\"weights/WEIGHT_SCALED_42840.npy\", WEIGHT_SCALED_42840)\n",
    "np.save(\"weights/WEIGHT_SCALED_30490.npy\", WEIGHT_SCALED_30490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T11:23:19.091711Z",
     "start_time": "2020-07-08T11:23:19.087903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.67637554,  9.75957465,  9.79967291, ...,  5.21888784,\n",
       "        4.90879915,  4.89985238])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHT_SCALED_30490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight (by Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T13:37:04.800069Z",
     "start_time": "2020-07-04T13:34:01.010622Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n",
      "Mem. usage decreased to 160.31 Mb (30.0% reduction)\n",
      "重みマップ作成・・・・\n",
      "ロス関数重み計算・・・・\n"
     ]
    }
   ],
   "source": [
    "################################# Make WEIGHT\n",
    "#################################################################################\n",
    "\n",
    "############## LOAD ORIGIN DATA\n",
    "\n",
    "train_val_df_origin = pd.read_csv('m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "sell_price_df = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "\n",
    "#############  Make Weights\n",
    "day_list = []\n",
    "for i in range(1942, 1970):\n",
    "    day_list.append(\"d_{}\".format(i))\n",
    "for day in day_list:\n",
    "    train_val_df_origin[day] = np.nan\n",
    "\n",
    "for category in CATEGORY_ID:\n",
    "    \n",
    "    train_val_df = train_val_df_origin[train_val_df_origin[f\"{category_name}_id\"]==category]\n",
    "\n",
    "\n",
    "    melt_sales = pd.melt(train_val_df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name=\"day\", value_name=\"sales\")\n",
    "    calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%Y-%m-%d')\n",
    "    melt_sales = melt_sales.merge(calendar_df, left_on=\"day\", right_on=\"d\")\n",
    "    melt_sales = pd.merge(melt_sales, sell_price_df, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')\n",
    "\n",
    "    melt_sales = melt_sales[[\"id\", \"date\", \"sales\", \"sell_price\"]]\n",
    "\n",
    "    melt_sales = reduce_mem_usage(melt_sales)\n",
    "\n",
    "    ############## カテゴリと州ごとのデータフレーム作成 \n",
    "    # train_val_df = train_val_df[train_val_df[\"distribution\"]==\"poisson\"]\n",
    "\n",
    "    NUM_ITEMS = train_val_df.shape[0]\n",
    "\n",
    "    product = train_val_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "    # 重みマップ作成\n",
    "    print(\"重みマップ作成・・・・\")\n",
    "    weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n",
    "                       pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                       np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n",
    "                       ].T\n",
    "\n",
    "    np.save(f\"weights/weight_mat_{category}.npy\" ,weight_mat)\n",
    "    weight_mat_csr = csr_matrix(weight_mat)\n",
    "\n",
    "#     del train_val_df, sell_price_df, calendar_df, weight_mat\n",
    "#     gc.collect()\n",
    "\n",
    "    ############## ロス関数の重み算出、保存\n",
    "    print(\"ロス関数重み計算・・・・\")\n",
    "    weight1, weight2 = weight_calc(melt_sales, product, category)\n",
    "    # del df, train_val_df; gc.collect()\n",
    "#     del melt_sales; gc.collect()\n",
    "\n",
    "    ############## SAVE WEIGHT\n",
    "\n",
    "    np.save(f\"weights/weight1_{category}.npy\" ,weight1)\n",
    "    np.save(f\"weights/weight2_{category}.npy\" ,weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T05:35:59.880900Z",
     "start_time": "2020-06-10T05:35:46.053172Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################################# LOAD WEIGHT\n",
    "#################################################################################\n",
    "\n",
    "# weight1 = np.load(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/weight/weight1.npy\")\n",
    "# weight2 = np.load(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/weight/weight2.npy\")\n",
    "# weight_mat = np.load(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/weight/weight_mat.npy\")\n",
    "# weight_mat_csr = csr_matrix(weight_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T03:02:37.415070Z",
     "start_time": "2020-06-01T03:02:37.412659Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "######## obj_wrmsseと使用する際に必要\n",
    "\n",
    "# LEVEL = {\"LEVEL1\": ['total_id'],\n",
    "#              \"LEVEL2\": [\"state_id\"],\n",
    "#              \"LEVEL3\": [\"store_id\"],\n",
    "#              \"LEVEL4\": [\"cat_id\"],\n",
    "#              \"LEVEL5\": [\"dept_id\"],\n",
    "#              \"LEVEL6\": [\"state_id_&_cat_id\"],\n",
    "#              \"LEVEL7\": [\"state_id_&_dept_id\"],\n",
    "#              \"LEVEL8\": [\"store_id_&_cat_id\"],\n",
    "#              \"LEVEL9\": [\"store_id_&_dept_id\"],\n",
    "#              \"LEVEL10\": [\"item_id\"],\n",
    "#              \"LEVEL11\": [\"state_id_&_item_id\"],\n",
    "#              \"LEVEL12\": [\"store_id_&_item_id\"]}\n",
    "\n",
    "# df_fm = cost_format()\n",
    "# weight_df1, w1 = w_df(weight1, df_fm)\n",
    "# weight_df2, w2 = w_df(weight2, df_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T14:00:16.589099Z",
     "start_time": "2020-07-04T13:54:43.929088Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic/\n",
      "Encoding/\n",
      "Mem. usage decreased to 15172.22 Mb (42.6% reduction)\n",
      "DataFrame\n"
     ]
    }
   ],
   "source": [
    "################################# Make DataFrame\n",
    "#################################################################################\n",
    "for term in [\"private\", \"public\", \"validation\", \"semival\"]:\n",
    "    \n",
    "    ##Basic\n",
    "    #folder select\n",
    "    main_path = \"features/\"\n",
    "    folder1 = \"basic/\" \n",
    "    folder2 = \"Encoding/\"\n",
    "    folder3 = \"Price/\"\n",
    "    folder4 = \"Lag_Features/\"\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    ######file select\n",
    "    ## BASIC\n",
    "    flist1 = [\n",
    "             'month.pkl',\n",
    "             'day.pkl',\n",
    "             'price_momentum_y_item_store.pkl',\n",
    "#              'w_serial.pkl',\n",
    "             'item_id.pkl',\n",
    "#              'd_serial.pkl',\n",
    "#              'snap_TX.pkl',\n",
    "#              '.DS_Store',\n",
    "             'last_sales.pkl',\n",
    "#              'snap_CA.pkl',\n",
    "             'is_weekend.pkl',\n",
    "#              'sales.pkl',\n",
    "             'sell_start_log.pkl',\n",
    "             'sell_price_minority12.pkl',\n",
    "             'week_of_year.pkl',\n",
    "             'price_momentum_m_item_state.pkl',\n",
    "             'Mothers_day.pkl',\n",
    "             'year.pkl',\n",
    "             'IndependenceDay.pkl',\n",
    "             'olympic_president_elec_year.pkl',\n",
    "             'event_name_1.pkl',\n",
    "             'dept_id.pkl',\n",
    "             'event_name_2.pkl',\n",
    "             'price_unique_item_state.pkl',\n",
    "             'date.pkl',\n",
    "             'id.pkl',\n",
    "             'moon.pkl',\n",
    "             'sell_price.pkl',\n",
    "             'national_holiday.pkl',\n",
    "             'state_id.pkl',\n",
    "#              'event_type_1.pkl',\n",
    "             'price_momentum_y_item_state.pkl',\n",
    "             'event_type_statecat_labelenc.pkl',\n",
    "             'event_type_2.pkl',\n",
    "             'store_id.pkl',\n",
    "             'snap_total.pkl',\n",
    "             'cat_id.pkl',\n",
    "#              'snap_WI.pkl',\n",
    "             'price_unique_item_store.pkl',\n",
    "             'sell_start.pkl',\n",
    "#              'weekday.pkl',\n",
    "             'week_of_month.pkl',\n",
    "             'Easter.pkl',\n",
    "             'wday.pkl',\n",
    "             'price_momentum_m_item_store.pkl',\n",
    "             'OrthodoxEaster.pkl',\n",
    "             'id_serial.pkl',\n",
    "             'Ramadan_Starts.pkl',\n",
    "             'NBA_finals.pkl',\n",
    "#              'wm_yr_wk.pkl',\n",
    "#              'd.pkl',\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Encoding\n",
    "    flist2 = []\n",
    "    for level in [f\"LEVEL{i}\" for i in range(2, 13)]:\n",
    "        flist2.append(f\"{term}_sales_residual_diff_28_roll_365_enc_{level}_mean.pkl\")\n",
    "        flist2.append(f\"{term}_sales_residual_diff_28_roll_365_enc_{level}_std.pkl\")\n",
    "    for diff in [28]:\n",
    "        for level in [f\"LEVEL{i}\" for i in range(2, 13)]:\n",
    "            for wd in [\"week\", \"day\"]:\n",
    "                for func in [\"mean\", \"std\"]:\n",
    "                    flist2.append(f\"{term}_sales_residual_diff_{diff}_roll_365_enc_{wd}_{level}_{func}.pkl\")\n",
    "\n",
    "\n",
    "    folders = [\n",
    "        folder1,\n",
    "        folder2, \n",
    "#         folder3,\n",
    "    #     folder4,\n",
    "    ]\n",
    "\n",
    "    flists = [\n",
    "        flist1,\n",
    "        flist2,\n",
    "#         flist3,\n",
    "    #     flist4,\n",
    "    ]\n",
    "\n",
    "    #DATAFRAME作成\n",
    "    for folder, flist in zip(folders, flists):\n",
    "        for filename in flist:\n",
    "            row = pd.read_pickle(main_path + folder + filename)\n",
    "            filename = filename[:-4]\n",
    "            df[filename] = row\n",
    "        print(\"{}\".format(folder))\n",
    "\n",
    "    sales = pd.read_pickle(\"features/Target/sales_residual_diff_28_roll_365.pkl\")\n",
    "    df[\"sales\"] = sales\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    order = ['id_serial', 'id', 'is_weekend', 'sell_start', 'date', 'snap_total', 'day', 'sell_price', 'event_name_1', 'week_of_month', 'wday', 'week_of_year',\n",
    "            'sell_start_log', 'Mothers_day', 'national_holiday', 'NBA_finals', 'sell_price_minority12', 'year', 'month', 'olympic_president_elec_year',\n",
    "            'OrthodoxEaster', 'store_id', 'moon', 'cat_id', 'Ramadan_Starts', 'IndependenceDay', 'last_sales', 'event_name_2','event_type_2', \n",
    "            'event_type_statecat_labelenc', 'Easter', 'item_id', 'dept_id', 'state_id',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL2_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL2_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL3_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL3_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL4_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL4_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL5_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL5_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL6_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL6_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL7_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL7_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL8_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL8_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL9_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL9_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL10_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL10_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL11_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL11_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL12_mean', f'{term}_sales_residual_diff_28_roll_365_enc_LEVEL12_std',\n",
    "            'price_unique_item_state', 'price_momentum_m_item_state', 'price_momentum_y_item_state',\n",
    "            'price_unique_item_store', 'price_momentum_m_item_store', 'price_momentum_y_item_store',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL2_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL2_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL2_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL2_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL3_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL3_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL3_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL3_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL4_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL4_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL4_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL4_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL5_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL5_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL5_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL5_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL6_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL6_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL6_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL6_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL7_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL7_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL7_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL7_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL8_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL8_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL8_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL8_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL9_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL9_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL9_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL9_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL10_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL10_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL10_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL10_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL11_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL11_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL11_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL11_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL12_mean', f'{term}_sales_residual_diff_28_roll_365_enc_week_LEVEL12_std',\n",
    "            f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL12_mean', f'{term}_sales_residual_diff_28_roll_365_enc_day_LEVEL12_std',\n",
    "            'sales',]\n",
    "    \n",
    "    df = df[order]\n",
    "\n",
    "    df.to_pickle(f\"dataframe/data_base_{term}_df.pkl\")\n",
    "#     df.to_pickle(f\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/data_day{SHIFT_DAY}_df.pkl\")\n",
    "\n",
    "    print(\"DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T14:11:39.439107Z",
     "start_time": "2020-07-04T14:11:30.147649Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features/Lag_Features/multi_7_sales_residual_diff_28_roll_365_shift_7_roll_2_mean.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9fa2d17eb034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features/Lag_Features/multi_7_sales_residual_diff_28_roll_365_shift_7_roll_2_mean.pkl'"
     ]
    }
   ],
   "source": [
    "################################# Make DataFrame\n",
    "#################################################################################\n",
    "for SHIFT_DAY in range(1, 29):\n",
    "    \n",
    "    if (SHIFT_DAY >=1) & (SHIFT_DAY<=7):\n",
    "        LAG_DAY = 7\n",
    "    elif (SHIFT_DAY >=8) & (SHIFT_DAY<=14):\n",
    "        LAG_DAY = 14\n",
    "    elif (SHIFT_DAY >=15) & (SHIFT_DAY<=21):\n",
    "        LAG_DAY = 21\n",
    "    elif (SHIFT_DAY >=22) & (SHIFT_DAY<=28):\n",
    "        LAG_DAY = 28\n",
    "    \n",
    "    ##Basic\n",
    "    #folder select\n",
    "    main_path = \"features/\"\n",
    "    folder1 = \"basic/\" \n",
    "    folder2 = \"Encoding/\"\n",
    "    folder3 = \"Price/\"\n",
    "    folder4 = \"Lag_Features/\"\n",
    "\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    ######file select\n",
    "    ## BASIC\n",
    "    \n",
    "    if SHIFT_DAY in [7, 14, 21, 28]:\n",
    "        flist4 = []\n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [2, 3, 4, 8, 12]:\n",
    "                for func in [\"mean\",]:\n",
    "                    flist4.append(f\"multi_7_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_{func}.pkl\")\n",
    "                    \n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [4, 8,]:\n",
    "                for func in [\"max\", \"min\"]:\n",
    "                    flist4.append(f\"multi_7_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_{func}.pkl\")\n",
    "                    \n",
    "\n",
    "        for shift in [SHIFT_DAY,]:\n",
    "            for multi in [2, 3, 5]:\n",
    "                for roll in [3, 6, 10]:\n",
    "                    for func in [\"mean\",]:\n",
    "                        flist4.append(f\"multi_{multi}_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_{func}.pkl\")      \n",
    "                        \n",
    "                        \n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 14, 30, 60]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_mean.pkl\")\n",
    "                \n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_std.pkl\")\n",
    "                \n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_median.pkl\")\n",
    "                \n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_max.pkl\")\n",
    "        \n",
    "        for shift in [LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_min.pkl\")\n",
    "                            \n",
    "        s = SHIFT_DAY\n",
    "        for shift in [s, s+1, s+2, s+3, s+4, s+5, s+6, s+7, s+8, s+9, s+10, s+11, s+12, s+13, ]:\n",
    "            flist4.append(f\"sales_residual_diff_28_roll_365_shift_{shift}.pkl\")\n",
    "    else:\n",
    "        \n",
    "        flist4 = []\n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [2, 3, 4, 8, 12]:\n",
    "                for func in [\"mean\",]:\n",
    "                    flist4.append(f\"multi_7_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_{func}.pkl\")\n",
    "                    \n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [4, 8,]:\n",
    "                for func in [\"max\", \"min\"]:\n",
    "                    flist4.append(f\"multi_7_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_{func}.pkl\")\n",
    "                    \n",
    "\n",
    "        for shift in [SHIFT_DAY,]:\n",
    "            for multi in [2, 3, 5]:\n",
    "                for roll in [3, 6, 10]:\n",
    "                    for func in [\"mean\",]:\n",
    "                        flist4.append(f\"multi_{multi}_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_{func}.pkl\")      \n",
    "                        \n",
    "                        \n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 14, 30, 60]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_mean.pkl\")\n",
    "                \n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_std.pkl\")\n",
    "                \n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_median.pkl\")\n",
    "                \n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_max.pkl\")\n",
    "        \n",
    "        for shift in [SHIFT_DAY, LAG_DAY, LAG_DAY+7]:\n",
    "            for roll in [7, 30]:\n",
    "                flist4.append(f\"multi_1_sales_residual_diff_28_roll_365_shift_{shift}_roll_{roll}_min.pkl\")\n",
    "                            \n",
    "        s = SHIFT_DAY\n",
    "        for shift in [s, s+1, s+2, s+3, s+4, s+5, s+6, s+7, s+8, s+9, s+10, s+11, s+12, s+13, ]:\n",
    "            flist4.append(f\"sales_residual_diff_28_roll_365_shift_{shift}.pkl\")\n",
    "\n",
    "\n",
    "    folders = [\n",
    "#         folder1,\n",
    "#         folder2, \n",
    "#         folder3,\n",
    "        folder4,  \n",
    "    ]\n",
    "\n",
    "    flists = [\n",
    "#         flist1,\n",
    "#         flist2,\n",
    "#         flist3,\n",
    "        flist4,\n",
    "    ]\n",
    "\n",
    "\n",
    "    #DATAFRAME作成\n",
    "    for folder, flist in zip(folders, flists):\n",
    "        for filename in flist:\n",
    "            row = pd.read_pickle(main_path + folder + filename)\n",
    "            filename = filename[:-4]\n",
    "            df[filename] = row\n",
    "        print(\"{}\".format(folder))\n",
    "\n",
    "#     sales = pd.read_pickle(\"features/Target/sales_residual_diff_28_roll_365.pkl\")\n",
    "#     df[\"sales\"] = sales\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "#     df.to_pickle(f\"dataframe/data_base_{term}_df.pkl\")\n",
    "    df.to_pickle(f\"dataframe/data_day{SHIFT_DAY}_df.pkl\")\n",
    "\n",
    "    print(\"DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T08:03:08.547130Z",
     "start_time": "2020-06-13T08:02:55.330517Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[\"start_sell_ym\"] = pd.read_pickle(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/basic/start_sell_ym.pkl\")\n",
    "# df[\"week_of_month\"] = pd.read_pickle(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/basic/week_of_month.pkl\")\n",
    "# df[\"week_of_year\"] = pd.read_pickle(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/ALL/basic/week_of_year.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:26:39.557908Z",
     "start_time": "2020-05-28T12:18:50.253Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################################## confirm feature\n",
    "# import pandas as pd\n",
    "# mf = pd.read_pickle(\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/model_features/model_features130.pkl\")\n",
    "\n",
    "# for i in mf.values:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T21:41:37.922498Z",
     "start_time": "2020-06-15T21:41:15.229105Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "################################# LOAD DATA\n",
    "#################################################################################\n",
    "\n",
    "# print(\"load data\")\n",
    "# base_df = pd.read_pickle(\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/data_base_df.pkl\")\n",
    "# lag_df = pd.read_pickle(f\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/data_day{SHIFT_DAY}_df.pkl\")\n",
    "# df = pd.concat((base_df, lag_df), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T16:12:02.446565Z",
     "start_time": "2020-07-06T16:12:02.438318Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##################### Model Params\n",
    "####################################################################\n",
    "\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': \"regression\",\n",
    "#             'alpha':0.5,\n",
    "#             'tweedie_variance_power': 1.1,\n",
    "#             \"force_row_wise\" : True,\n",
    "#             'max_depth':11,\n",
    "            'metric': 'custom',\n",
    "#             'metric': '[rmse]',\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "#             'learning_rate': 0.01,\n",
    "            'learning_rate': 0.08,\n",
    "            'num_leaves': 2**11-1,   \n",
    "            'min_data_in_leaf': 2**12-1,\n",
    "#             \"lambda_l2\" : 0.1,\n",
    "            'feature_fraction': 0.8,\n",
    "            'max_bin': 255,     \n",
    "            'n_estimators': 1500,\n",
    "            'boost_from_average': False,\n",
    "            'verbose': -1,\n",
    "#             \"is_enable_sparse\": False,\n",
    "              }\n",
    "def default_params():\n",
    "        params[\"subsample\"] = 0.5\n",
    "        params[\"learning_rate\"] = 0.01\n",
    "#         params[\"learning_rate\"] = 0.08\n",
    "        params[\"num_leaves\"] = 2**11-1\n",
    "        params[\"min_data_in_leaf\"] = 2**12-1\n",
    "        params[\"feature_fraction\"] = 0.8\n",
    "        params[\"max_bin\"] = 255\n",
    "        params[\"n_estimators\"] =1500\n",
    "        \n",
    "def category_param(category):\n",
    "    default_params()\n",
    "    if category==\"CA_1\":\n",
    "#         params[\"n_estimators\"] =1200\n",
    "        pass\n",
    "\n",
    "    elif category==\"CA_2\":\n",
    "        params[\"num_leaves\"] = 2**8-1\n",
    "        params[\"min_data_in_leaf\"] = 2**8-1\n",
    "        \n",
    "    elif category==\"CA_3\":\n",
    "#         params[\"learning_rate\"] = 0.03\n",
    "        params[\"num_leaves\"] = 2**8-1\n",
    "        params[\"min_data_in_leaf\"] = 2**8-1\n",
    "        params[\"n_estimators\"] =2300\n",
    "        \n",
    "    elif category==\"CA_4\":\n",
    "        params[\"feature_fraction\"] = 0.5\n",
    "        \n",
    "    elif category==\"TX_1\":\n",
    "        pass\n",
    "    elif category==\"TX_2\":\n",
    "        pass\n",
    "    elif category==\"TX_3\":\n",
    "        pass\n",
    "    elif category==\"WI_1\":\n",
    "        pass\n",
    "    elif category==\"WI_2\":\n",
    "        params[\"num_leaves\"] = 2**8-1\n",
    "        params[\"min_data_in_leaf\"] = 2**8-1\n",
    "        params[\"feature_fraction\"] = 0.5\n",
    "    elif category==\"WI_3\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T03:51:29.554324Z",
     "start_time": "2020-07-05T03:51:08.373416Z"
    }
   },
   "outputs": [],
   "source": [
    "############### pred format\n",
    "#################################################\n",
    "\n",
    "ID = pd.read_pickle(\"features/Basic/id.pkl\")\n",
    "store_id = pd.read_pickle(\"features/Basic/store_id.pkl\")\n",
    "d_serial = pd.read_pickle(\"features/Basic/d_serial.pkl\")\n",
    "shift_28_rolling_365 = pd.read_pickle(\"features/Target/shift_28_roll_365.pkl\")\n",
    "target= pd.read_pickle(\"features/Target/sales_residual_diff_28_roll_365.pkl\")\n",
    "predictions = pd.DataFrame({\"id\":ID,\n",
    "                             \"store_id\":store_id,\n",
    "                             \"d_serial\":d_serial,\n",
    "                             \"shift_28_rolling_365\":shift_28_rolling_365,\n",
    "                             \"target\":target})\n",
    "\n",
    "predictions[\"total_id\"] = \"total\"\n",
    "predictions[\"pred\"] = 0\n",
    "\n",
    "pred_terms = [\"private\", \"public\", \"validation\", \"semival\"]\n",
    "\n",
    "for i, term in enumerate(pred_terms):\n",
    "    DAYS_COEF = i\n",
    "    predictions_term = predictions.loc[(predictions.d_serial >= 1942-28*DAYS_COEF)&(predictions.d_serial <= 1969-28*DAYS_COEF)]\n",
    "    predictions_term.to_pickle(f\"prediction/format/{term}_pred_fm.pkl\")\n",
    "\n",
    "\n",
    "val_terms = [\"public\", \"validation\", \"semival\"]\n",
    "for term in val_terms:\n",
    "    for i in [1, 7, 14, 21, 28]:\n",
    "        predictions_term = pd.read_pickle(f\"prediction/format/{term}_pred_fm.pkl\")\n",
    "        predictions_term.to_pickle(f\"prediction/{term}_pred_shift{i}.pkl\")\n",
    "\n",
    "predictions_term = pd.read_pickle(f\"prediction/format/private_pred_fm.pkl\")\n",
    "predictions_term.to_pickle(f\"prediction/private_pred.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T01:14:56.343200Z",
     "start_time": "2020-07-06T16:24:25.547644Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----day1-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.43107\tvalid_1's wrmsse: 0.497817\n",
      "[200]\ttraining's wrmsse: 0.43184\tvalid_1's wrmsse: 0.491939\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's wrmsse: 0.43327\tvalid_1's wrmsse: 0.488731\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.450458\tvalid_1's wrmsse: 0.572981\n",
      "[200]\ttraining's wrmsse: 0.427557\tvalid_1's wrmsse: 0.570384\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's wrmsse: 0.437061\tvalid_1's wrmsse: 0.565589\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.480926\tvalid_1's wrmsse: 0.536592\n",
      "[200]\ttraining's wrmsse: 0.469448\tvalid_1's wrmsse: 0.524623\n",
      "[300]\ttraining's wrmsse: 0.4635\tvalid_1's wrmsse: 0.52496\n",
      "Early stopping, best iteration is:\n",
      "[236]\ttraining's wrmsse: 0.46648\tvalid_1's wrmsse: 0.520893\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.576285\tvalid_1's wrmsse: 0.654364\n",
      "[200]\ttraining's wrmsse: 0.566029\tvalid_1's wrmsse: 0.646359\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttraining's wrmsse: 0.569605\tvalid_1's wrmsse: 0.644356\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.481029\tvalid_1's wrmsse: 0.656607\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's wrmsse: 0.47387\tvalid_1's wrmsse: 0.652395\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.473733\tvalid_1's wrmsse: 0.546896\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's wrmsse: 0.472932\tvalid_1's wrmsse: 0.538972\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.537483\tvalid_1's wrmsse: 0.666399\n",
      "[200]\ttraining's wrmsse: 0.516081\tvalid_1's wrmsse: 0.650852\n",
      "[300]\ttraining's wrmsse: 0.504777\tvalid_1's wrmsse: 0.647273\n",
      "[400]\ttraining's wrmsse: 0.49719\tvalid_1's wrmsse: 0.643501\n",
      "Early stopping, best iteration is:\n",
      "[387]\ttraining's wrmsse: 0.499643\tvalid_1's wrmsse: 0.641345\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.506969\tvalid_1's wrmsse: 0.539428\n",
      "[200]\ttraining's wrmsse: 0.491072\tvalid_1's wrmsse: 0.523856\n",
      "[300]\ttraining's wrmsse: 0.484286\tvalid_1's wrmsse: 0.520266\n",
      "[400]\ttraining's wrmsse: 0.476219\tvalid_1's wrmsse: 0.520955\n",
      "Early stopping, best iteration is:\n",
      "[300]\ttraining's wrmsse: 0.484286\tvalid_1's wrmsse: 0.520266\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.627853\tvalid_1's wrmsse: 0.729214\n",
      "[200]\ttraining's wrmsse: 0.602483\tvalid_1's wrmsse: 0.716019\n",
      "[300]\ttraining's wrmsse: 0.580018\tvalid_1's wrmsse: 0.709098\n",
      "[400]\ttraining's wrmsse: 0.56613\tvalid_1's wrmsse: 0.704193\n",
      "[500]\ttraining's wrmsse: 0.553869\tvalid_1's wrmsse: 0.703486\n",
      "[600]\ttraining's wrmsse: 0.542736\tvalid_1's wrmsse: 0.701381\n",
      "[700]\ttraining's wrmsse: 0.530808\tvalid_1's wrmsse: 0.699094\n",
      "[800]\ttraining's wrmsse: 0.520707\tvalid_1's wrmsse: 0.696762\n",
      "[900]\ttraining's wrmsse: 0.514073\tvalid_1's wrmsse: 0.695669\n",
      "Early stopping, best iteration is:\n",
      "[893]\ttraining's wrmsse: 0.514414\tvalid_1's wrmsse: 0.694955\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.440217\tvalid_1's wrmsse: 0.559696\n",
      "[200]\ttraining's wrmsse: 0.430643\tvalid_1's wrmsse: 0.550171\n",
      "[300]\ttraining's wrmsse: 0.419993\tvalid_1's wrmsse: 0.545697\n",
      "[400]\ttraining's wrmsse: 0.41082\tvalid_1's wrmsse: 0.543527\n",
      "[500]\ttraining's wrmsse: 0.404961\tvalid_1's wrmsse: 0.544115\n",
      "Early stopping, best iteration is:\n",
      "[413]\ttraining's wrmsse: 0.410332\tvalid_1's wrmsse: 0.542726\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day7-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.474929\tvalid_1's wrmsse: 0.505728\n",
      "[200]\ttraining's wrmsse: 0.462779\tvalid_1's wrmsse: 0.492744\n",
      "[300]\ttraining's wrmsse: 0.452211\tvalid_1's wrmsse: 0.488958\n",
      "[400]\ttraining's wrmsse: 0.444716\tvalid_1's wrmsse: 0.487256\n",
      "[500]\ttraining's wrmsse: 0.43476\tvalid_1's wrmsse: 0.480532\n",
      "[600]\ttraining's wrmsse: 0.421826\tvalid_1's wrmsse: 0.478254\n",
      "[700]\ttraining's wrmsse: 0.415916\tvalid_1's wrmsse: 0.479338\n",
      "Early stopping, best iteration is:\n",
      "[678]\ttraining's wrmsse: 0.4163\tvalid_1's wrmsse: 0.476424\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.478015\tvalid_1's wrmsse: 0.574169\n",
      "[200]\ttraining's wrmsse: 0.459977\tvalid_1's wrmsse: 0.562213\n",
      "[300]\ttraining's wrmsse: 0.44369\tvalid_1's wrmsse: 0.574266\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttraining's wrmsse: 0.460242\tvalid_1's wrmsse: 0.561203\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.553004\tvalid_1's wrmsse: 0.59899\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's wrmsse: 0.548667\tvalid_1's wrmsse: 0.572381\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.596824\tvalid_1's wrmsse: 0.671545\n",
      "[200]\ttraining's wrmsse: 0.585295\tvalid_1's wrmsse: 0.654617\n",
      "[300]\ttraining's wrmsse: 0.574141\tvalid_1's wrmsse: 0.654307\n",
      "[400]\ttraining's wrmsse: 0.565894\tvalid_1's wrmsse: 0.651953\n",
      "Early stopping, best iteration is:\n",
      "[341]\ttraining's wrmsse: 0.572734\tvalid_1's wrmsse: 0.64956\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.516355\tvalid_1's wrmsse: 0.673305\n",
      "[200]\ttraining's wrmsse: 0.501818\tvalid_1's wrmsse: 0.661418\n",
      "[300]\ttraining's wrmsse: 0.491195\tvalid_1's wrmsse: 0.655321\n",
      "[400]\ttraining's wrmsse: 0.480098\tvalid_1's wrmsse: 0.655015\n",
      "[500]\ttraining's wrmsse: 0.473125\tvalid_1's wrmsse: 0.653359\n",
      "[600]\ttraining's wrmsse: 0.46738\tvalid_1's wrmsse: 0.648125\n",
      "[700]\ttraining's wrmsse: 0.456836\tvalid_1's wrmsse: 0.647567\n",
      "Early stopping, best iteration is:\n",
      "[697]\ttraining's wrmsse: 0.457263\tvalid_1's wrmsse: 0.647219\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.51585\tvalid_1's wrmsse: 0.535004\n",
      "[200]\ttraining's wrmsse: 0.494282\tvalid_1's wrmsse: 0.527405\n",
      "Early stopping, best iteration is:\n",
      "[188]\ttraining's wrmsse: 0.496121\tvalid_1's wrmsse: 0.526928\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.572065\tvalid_1's wrmsse: 0.670585\n",
      "[200]\ttraining's wrmsse: 0.559554\tvalid_1's wrmsse: 0.655886\n",
      "[300]\ttraining's wrmsse: 0.550263\tvalid_1's wrmsse: 0.652021\n",
      "[400]\ttraining's wrmsse: 0.542037\tvalid_1's wrmsse: 0.653655\n",
      "Early stopping, best iteration is:\n",
      "[388]\ttraining's wrmsse: 0.544131\tvalid_1's wrmsse: 0.648688\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.534725\tvalid_1's wrmsse: 0.546247\n",
      "[200]\ttraining's wrmsse: 0.520145\tvalid_1's wrmsse: 0.523165\n",
      "[300]\ttraining's wrmsse: 0.510673\tvalid_1's wrmsse: 0.522541\n",
      "[400]\ttraining's wrmsse: 0.499681\tvalid_1's wrmsse: 0.519939\n",
      "[500]\ttraining's wrmsse: 0.49285\tvalid_1's wrmsse: 0.516428\n",
      "[600]\ttraining's wrmsse: 0.481877\tvalid_1's wrmsse: 0.517893\n",
      "Early stopping, best iteration is:\n",
      "[533]\ttraining's wrmsse: 0.489685\tvalid_1's wrmsse: 0.516261\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.693507\tvalid_1's wrmsse: 0.787135\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's wrmsse: 0.709579\tvalid_1's wrmsse: 0.776768\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.474801\tvalid_1's wrmsse: 0.553467\n",
      "[200]\ttraining's wrmsse: 0.46857\tvalid_1's wrmsse: 0.556793\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttraining's wrmsse: 0.475097\tvalid_1's wrmsse: 0.551292\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day14-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.474258\tvalid_1's wrmsse: 0.518231\n",
      "[200]\ttraining's wrmsse: 0.470504\tvalid_1's wrmsse: 0.494409\n",
      "[300]\ttraining's wrmsse: 0.464959\tvalid_1's wrmsse: 0.48929\n",
      "[400]\ttraining's wrmsse: 0.454928\tvalid_1's wrmsse: 0.485342\n",
      "Early stopping, best iteration is:\n",
      "[397]\ttraining's wrmsse: 0.454958\tvalid_1's wrmsse: 0.485153\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.486306\tvalid_1's wrmsse: 0.589679\n",
      "[200]\ttraining's wrmsse: 0.46657\tvalid_1's wrmsse: 0.580376\n",
      "[300]\ttraining's wrmsse: 0.456746\tvalid_1's wrmsse: 0.582944\n",
      "Early stopping, best iteration is:\n",
      "[234]\ttraining's wrmsse: 0.465974\tvalid_1's wrmsse: 0.577158\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.581239\tvalid_1's wrmsse: 0.663833\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's wrmsse: 0.584986\tvalid_1's wrmsse: 0.609327\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.596563\tvalid_1's wrmsse: 0.693526\n",
      "[200]\ttraining's wrmsse: 0.590049\tvalid_1's wrmsse: 0.669803\n",
      "[300]\ttraining's wrmsse: 0.579896\tvalid_1's wrmsse: 0.660319\n",
      "[400]\ttraining's wrmsse: 0.565845\tvalid_1's wrmsse: 0.660864\n",
      "Early stopping, best iteration is:\n",
      "[338]\ttraining's wrmsse: 0.575294\tvalid_1's wrmsse: 0.657599\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.530566\tvalid_1's wrmsse: 0.667518\n",
      "[200]\ttraining's wrmsse: 0.519024\tvalid_1's wrmsse: 0.658528\n",
      "[300]\ttraining's wrmsse: 0.505417\tvalid_1's wrmsse: 0.656158\n",
      "[400]\ttraining's wrmsse: 0.491712\tvalid_1's wrmsse: 0.650512\n",
      "[500]\ttraining's wrmsse: 0.485057\tvalid_1's wrmsse: 0.650793\n",
      "Early stopping, best iteration is:\n",
      "[410]\ttraining's wrmsse: 0.488762\tvalid_1's wrmsse: 0.649384\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.534559\tvalid_1's wrmsse: 0.556043\n",
      "[200]\ttraining's wrmsse: 0.510447\tvalid_1's wrmsse: 0.543085\n",
      "[300]\ttraining's wrmsse: 0.490134\tvalid_1's wrmsse: 0.538678\n",
      "[400]\ttraining's wrmsse: 0.473758\tvalid_1's wrmsse: 0.539145\n",
      "Early stopping, best iteration is:\n",
      "[371]\ttraining's wrmsse: 0.480391\tvalid_1's wrmsse: 0.536719\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.607248\tvalid_1's wrmsse: 0.686557\n",
      "[200]\ttraining's wrmsse: 0.586397\tvalid_1's wrmsse: 0.674444\n",
      "[300]\ttraining's wrmsse: 0.575313\tvalid_1's wrmsse: 0.667862\n",
      "[400]\ttraining's wrmsse: 0.562895\tvalid_1's wrmsse: 0.663358\n",
      "Early stopping, best iteration is:\n",
      "[395]\ttraining's wrmsse: 0.563922\tvalid_1's wrmsse: 0.662515\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.520387\tvalid_1's wrmsse: 0.554561\n",
      "[200]\ttraining's wrmsse: 0.50339\tvalid_1's wrmsse: 0.536904\n",
      "[300]\ttraining's wrmsse: 0.493868\tvalid_1's wrmsse: 0.532396\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's wrmsse: 0.497074\tvalid_1's wrmsse: 0.530507\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.678506\tvalid_1's wrmsse: 0.77943\n",
      "[200]\ttraining's wrmsse: 0.650936\tvalid_1's wrmsse: 0.78585\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's wrmsse: 0.658954\tvalid_1's wrmsse: 0.769736\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.478846\tvalid_1's wrmsse: 0.580853\n",
      "[200]\ttraining's wrmsse: 0.473891\tvalid_1's wrmsse: 0.583946\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's wrmsse: 0.478804\tvalid_1's wrmsse: 0.580494\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day21-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.479793\tvalid_1's wrmsse: 0.520837\n",
      "[200]\ttraining's wrmsse: 0.470938\tvalid_1's wrmsse: 0.497498\n",
      "[300]\ttraining's wrmsse: 0.460455\tvalid_1's wrmsse: 0.490726\n",
      "[400]\ttraining's wrmsse: 0.445263\tvalid_1's wrmsse: 0.492767\n",
      "[500]\ttraining's wrmsse: 0.436205\tvalid_1's wrmsse: 0.490318\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttraining's wrmsse: 0.438654\tvalid_1's wrmsse: 0.489189\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.469021\tvalid_1's wrmsse: 0.586798\n",
      "[200]\ttraining's wrmsse: 0.467425\tvalid_1's wrmsse: 0.583573\n",
      "[300]\ttraining's wrmsse: 0.456422\tvalid_1's wrmsse: 0.599208\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttraining's wrmsse: 0.467595\tvalid_1's wrmsse: 0.582633\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.61324\tvalid_1's wrmsse: 0.699832\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's wrmsse: 0.614067\tvalid_1's wrmsse: 0.632122\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.609909\tvalid_1's wrmsse: 0.689603\n",
      "[200]\ttraining's wrmsse: 0.592769\tvalid_1's wrmsse: 0.674474\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's wrmsse: 0.600068\tvalid_1's wrmsse: 0.669656\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.521466\tvalid_1's wrmsse: 0.659748\n",
      "[200]\ttraining's wrmsse: 0.509327\tvalid_1's wrmsse: 0.645747\n",
      "[300]\ttraining's wrmsse: 0.495608\tvalid_1's wrmsse: 0.642131\n",
      "[400]\ttraining's wrmsse: 0.482723\tvalid_1's wrmsse: 0.640663\n",
      "[500]\ttraining's wrmsse: 0.470868\tvalid_1's wrmsse: 0.640292\n",
      "[600]\ttraining's wrmsse: 0.466811\tvalid_1's wrmsse: 0.637654\n",
      "[700]\ttraining's wrmsse: 0.459194\tvalid_1's wrmsse: 0.641556\n",
      "Early stopping, best iteration is:\n",
      "[613]\ttraining's wrmsse: 0.467078\tvalid_1's wrmsse: 0.63702\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.545802\tvalid_1's wrmsse: 0.56718\n",
      "[200]\ttraining's wrmsse: 0.522128\tvalid_1's wrmsse: 0.556203\n",
      "[300]\ttraining's wrmsse: 0.498833\tvalid_1's wrmsse: 0.554165\n",
      "[400]\ttraining's wrmsse: 0.485525\tvalid_1's wrmsse: 0.545558\n",
      "[500]\ttraining's wrmsse: 0.480593\tvalid_1's wrmsse: 0.544876\n",
      "[600]\ttraining's wrmsse: 0.469761\tvalid_1's wrmsse: 0.542486\n",
      "[700]\ttraining's wrmsse: 0.459905\tvalid_1's wrmsse: 0.54079\n",
      "Early stopping, best iteration is:\n",
      "[641]\ttraining's wrmsse: 0.468007\tvalid_1's wrmsse: 0.538789\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.635517\tvalid_1's wrmsse: 0.698541\n",
      "[200]\ttraining's wrmsse: 0.618039\tvalid_1's wrmsse: 0.686356\n",
      "[300]\ttraining's wrmsse: 0.597896\tvalid_1's wrmsse: 0.694588\n",
      "Early stopping, best iteration is:\n",
      "[226]\ttraining's wrmsse: 0.610235\tvalid_1's wrmsse: 0.684271\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.544153\tvalid_1's wrmsse: 0.551828\n",
      "[200]\ttraining's wrmsse: 0.527853\tvalid_1's wrmsse: 0.536576\n",
      "[300]\ttraining's wrmsse: 0.511561\tvalid_1's wrmsse: 0.532548\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's wrmsse: 0.520392\tvalid_1's wrmsse: 0.531414\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.6684\tvalid_1's wrmsse: 0.774602\n",
      "[200]\ttraining's wrmsse: 0.644413\tvalid_1's wrmsse: 0.778247\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's wrmsse: 0.649273\tvalid_1's wrmsse: 0.766276\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.495313\tvalid_1's wrmsse: 0.586326\n",
      "[200]\ttraining's wrmsse: 0.480869\tvalid_1's wrmsse: 0.585393\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's wrmsse: 0.483713\tvalid_1's wrmsse: 0.582557\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day28-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's wrmsse: 0.479527\tvalid_1's wrmsse: 0.538225\n",
      "[200]\ttraining's wrmsse: 0.469562\tvalid_1's wrmsse: 0.523015\n",
      "[300]\ttraining's wrmsse: 0.460787\tvalid_1's wrmsse: 0.516001\n",
      "[400]\ttraining's wrmsse: 0.444188\tvalid_1's wrmsse: 0.511007\n",
      "[500]\ttraining's wrmsse: 0.433086\tvalid_1's wrmsse: 0.506881\n",
      "[600]\ttraining's wrmsse: 0.425777\tvalid_1's wrmsse: 0.504747\n",
      "Early stopping, best iteration is:\n",
      "[563]\ttraining's wrmsse: 0.427963\tvalid_1's wrmsse: 0.50392\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.457566\tvalid_1's wrmsse: 0.592136\n",
      "[200]\ttraining's wrmsse: 0.453947\tvalid_1's wrmsse: 0.581793\n",
      "[300]\ttraining's wrmsse: 0.446867\tvalid_1's wrmsse: 0.581352\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's wrmsse: 0.449862\tvalid_1's wrmsse: 0.579021\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.647006\tvalid_1's wrmsse: 0.742575\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's wrmsse: 0.648882\tvalid_1's wrmsse: 0.655827\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.599647\tvalid_1's wrmsse: 0.696768\n",
      "[200]\ttraining's wrmsse: 0.594277\tvalid_1's wrmsse: 0.686811\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttraining's wrmsse: 0.59577\tvalid_1's wrmsse: 0.686058\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.544355\tvalid_1's wrmsse: 0.672472\n",
      "[200]\ttraining's wrmsse: 0.523451\tvalid_1's wrmsse: 0.660236\n",
      "[300]\ttraining's wrmsse: 0.506831\tvalid_1's wrmsse: 0.65881\n",
      "[400]\ttraining's wrmsse: 0.501168\tvalid_1's wrmsse: 0.652731\n",
      "[500]\ttraining's wrmsse: 0.48565\tvalid_1's wrmsse: 0.653037\n",
      "[600]\ttraining's wrmsse: 0.478557\tvalid_1's wrmsse: 0.652463\n",
      "[700]\ttraining's wrmsse: 0.466341\tvalid_1's wrmsse: 0.653257\n",
      "Early stopping, best iteration is:\n",
      "[623]\ttraining's wrmsse: 0.478157\tvalid_1's wrmsse: 0.652095\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.568543\tvalid_1's wrmsse: 0.577498\n",
      "[200]\ttraining's wrmsse: 0.53867\tvalid_1's wrmsse: 0.561523\n",
      "[300]\ttraining's wrmsse: 0.520933\tvalid_1's wrmsse: 0.558189\n",
      "[400]\ttraining's wrmsse: 0.506269\tvalid_1's wrmsse: 0.556004\n",
      "[500]\ttraining's wrmsse: 0.493312\tvalid_1's wrmsse: 0.555078\n",
      "[600]\ttraining's wrmsse: 0.480064\tvalid_1's wrmsse: 0.552316\n",
      "[700]\ttraining's wrmsse: 0.475262\tvalid_1's wrmsse: 0.550483\n",
      "[800]\ttraining's wrmsse: 0.467563\tvalid_1's wrmsse: 0.548227\n",
      "Early stopping, best iteration is:\n",
      "[777]\ttraining's wrmsse: 0.467078\tvalid_1's wrmsse: 0.547681\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.649104\tvalid_1's wrmsse: 0.703737\n",
      "[200]\ttraining's wrmsse: 0.62589\tvalid_1's wrmsse: 0.692082\n",
      "[300]\ttraining's wrmsse: 0.607542\tvalid_1's wrmsse: 0.691158\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's wrmsse: 0.611149\tvalid_1's wrmsse: 0.689625\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.53889\tvalid_1's wrmsse: 0.556858\n",
      "[200]\ttraining's wrmsse: 0.526217\tvalid_1's wrmsse: 0.536355\n",
      "[300]\ttraining's wrmsse: 0.512418\tvalid_1's wrmsse: 0.533612\n",
      "[400]\ttraining's wrmsse: 0.499448\tvalid_1's wrmsse: 0.534167\n",
      "[500]\ttraining's wrmsse: 0.488068\tvalid_1's wrmsse: 0.531984\n",
      "Early stopping, best iteration is:\n",
      "[499]\ttraining's wrmsse: 0.48811\tvalid_1's wrmsse: 0.53193\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.686571\tvalid_1's wrmsse: 0.759859\n",
      "[200]\ttraining's wrmsse: 0.6607\tvalid_1's wrmsse: 0.753328\n",
      "[300]\ttraining's wrmsse: 0.638787\tvalid_1's wrmsse: 0.752277\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttraining's wrmsse: 0.640478\tvalid_1's wrmsse: 0.751477\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.500161\tvalid_1's wrmsse: 0.567559\n",
      "[200]\ttraining's wrmsse: 0.486689\tvalid_1's wrmsse: 0.565368\n",
      "[300]\ttraining's wrmsse: 0.472139\tvalid_1's wrmsse: 0.566925\n",
      "[400]\ttraining's wrmsse: 0.453324\tvalid_1's wrmsse: 0.564645\n",
      "Early stopping, best iteration is:\n",
      "[353]\ttraining's wrmsse: 0.462105\tvalid_1's wrmsse: 0.562826\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day1-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.506023\tvalid_1's wrmsse: 0.444981\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's wrmsse: 0.538122\tvalid_1's wrmsse: 0.433441\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.924446\tvalid_1's wrmsse: 0.530731\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's wrmsse: 0.963271\tvalid_1's wrmsse: 0.512917\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.492646\tvalid_1's wrmsse: 0.513165\n",
      "[200]\ttraining's wrmsse: 0.473019\tvalid_1's wrmsse: 0.50799\n",
      "[300]\ttraining's wrmsse: 0.459082\tvalid_1's wrmsse: 0.509904\n",
      "Early stopping, best iteration is:\n",
      "[260]\ttraining's wrmsse: 0.461297\tvalid_1's wrmsse: 0.50385\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.589734\tvalid_1's wrmsse: 0.603254\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's wrmsse: 0.592544\tvalid_1's wrmsse: 0.601005\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.528437\tvalid_1's wrmsse: 0.500997\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's wrmsse: 0.529554\tvalid_1's wrmsse: 0.485947\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.514998\tvalid_1's wrmsse: 0.499343\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's wrmsse: 0.508805\tvalid_1's wrmsse: 0.483285\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.62633\tvalid_1's wrmsse: 0.571437\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's wrmsse: 0.603467\tvalid_1's wrmsse: 0.558317\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.650748\tvalid_1's wrmsse: 0.547146\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttraining's wrmsse: 0.675758\tvalid_1's wrmsse: 0.546114\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.619232\tvalid_1's wrmsse: 0.671049\n",
      "[200]\ttraining's wrmsse: 0.596798\tvalid_1's wrmsse: 0.667611\n",
      "[300]\ttraining's wrmsse: 0.581657\tvalid_1's wrmsse: 0.664574\n",
      "[400]\ttraining's wrmsse: 0.568292\tvalid_1's wrmsse: 0.664623\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's wrmsse: 0.57838\tvalid_1's wrmsse: 0.663687\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.528728\tvalid_1's wrmsse: 0.464826\n",
      "[200]\ttraining's wrmsse: 0.511672\tvalid_1's wrmsse: 0.46135\n",
      "[300]\ttraining's wrmsse: 0.503098\tvalid_1's wrmsse: 0.460725\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's wrmsse: 0.504073\tvalid_1's wrmsse: 0.459642\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day7-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.521985\tvalid_1's wrmsse: 0.492124\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.568463\tvalid_1's wrmsse: 0.472771\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.968794\tvalid_1's wrmsse: 0.578936\n",
      "[200]\ttraining's wrmsse: 0.858158\tvalid_1's wrmsse: 0.571907\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's wrmsse: 0.870968\tvalid_1's wrmsse: 0.566456\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.542943\tvalid_1's wrmsse: 0.58578\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's wrmsse: 0.554421\tvalid_1's wrmsse: 0.569627\n",
      "START_CA_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.608974\tvalid_1's wrmsse: 0.632489\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's wrmsse: 0.624752\tvalid_1's wrmsse: 0.621838\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.552236\tvalid_1's wrmsse: 0.541863\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's wrmsse: 0.554743\tvalid_1's wrmsse: 0.521854\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.518886\tvalid_1's wrmsse: 0.540192\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttraining's wrmsse: 0.512209\tvalid_1's wrmsse: 0.499541\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.637114\tvalid_1's wrmsse: 0.603088\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's wrmsse: 0.60642\tvalid_1's wrmsse: 0.568099\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.660229\tvalid_1's wrmsse: 0.575242\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's wrmsse: 0.700034\tvalid_1's wrmsse: 0.571834\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.639266\tvalid_1's wrmsse: 0.740494\n",
      "[200]\ttraining's wrmsse: 0.617629\tvalid_1's wrmsse: 0.72916\n",
      "[300]\ttraining's wrmsse: 0.599959\tvalid_1's wrmsse: 0.726655\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttraining's wrmsse: 0.601505\tvalid_1's wrmsse: 0.725599\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.550204\tvalid_1's wrmsse: 0.487711\n",
      "[200]\ttraining's wrmsse: 0.527931\tvalid_1's wrmsse: 0.486798\n",
      "[300]\ttraining's wrmsse: 0.514082\tvalid_1's wrmsse: 0.487898\n",
      "Early stopping, best iteration is:\n",
      "[274]\ttraining's wrmsse: 0.516999\tvalid_1's wrmsse: 0.485961\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day14-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.53006\tvalid_1's wrmsse: 0.492953\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's wrmsse: 0.568086\tvalid_1's wrmsse: 0.463877\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.929545\tvalid_1's wrmsse: 0.667253\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's wrmsse: 1.04172\tvalid_1's wrmsse: 0.617317\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.538897\tvalid_1's wrmsse: 0.610923\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's wrmsse: 0.550443\tvalid_1's wrmsse: 0.592445\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.595884\tvalid_1's wrmsse: 0.62354\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's wrmsse: 0.617714\tvalid_1's wrmsse: 0.61474\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.571705\tvalid_1's wrmsse: 0.557628\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's wrmsse: 0.570682\tvalid_1's wrmsse: 0.533031\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.545347\tvalid_1's wrmsse: 0.576758\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's wrmsse: 0.533879\tvalid_1's wrmsse: 0.520375\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.668885\tvalid_1's wrmsse: 0.642385\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's wrmsse: 0.63535\tvalid_1's wrmsse: 0.594639\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.651572\tvalid_1's wrmsse: 0.559751\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's wrmsse: 0.701924\tvalid_1's wrmsse: 0.555921\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.661837\tvalid_1's wrmsse: 0.734292\n",
      "[200]\ttraining's wrmsse: 0.6322\tvalid_1's wrmsse: 0.726419\n",
      "[300]\ttraining's wrmsse: 0.611081\tvalid_1's wrmsse: 0.725676\n",
      "[400]\ttraining's wrmsse: 0.597093\tvalid_1's wrmsse: 0.726412\n",
      "Early stopping, best iteration is:\n",
      "[308]\ttraining's wrmsse: 0.609098\tvalid_1's wrmsse: 0.724404\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.555224\tvalid_1's wrmsse: 0.505676\n",
      "[200]\ttraining's wrmsse: 0.536816\tvalid_1's wrmsse: 0.504597\n",
      "[300]\ttraining's wrmsse: 0.526338\tvalid_1's wrmsse: 0.501218\n",
      "[400]\ttraining's wrmsse: 0.510118\tvalid_1's wrmsse: 0.502401\n",
      "Early stopping, best iteration is:\n",
      "[315]\ttraining's wrmsse: 0.521886\tvalid_1's wrmsse: 0.500033\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day21-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.542373\tvalid_1's wrmsse: 0.483548\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's wrmsse: 0.579773\tvalid_1's wrmsse: 0.466724\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.947679\tvalid_1's wrmsse: 0.60262\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's wrmsse: 1.03726\tvalid_1's wrmsse: 0.567397\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.558739\tvalid_1's wrmsse: 0.651156\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's wrmsse: 0.556796\tvalid_1's wrmsse: 0.614898\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.600138\tvalid_1's wrmsse: 0.630707\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's wrmsse: 0.624999\tvalid_1's wrmsse: 0.621976\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.571169\tvalid_1's wrmsse: 0.544686\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's wrmsse: 0.574589\tvalid_1's wrmsse: 0.527191\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.546161\tvalid_1's wrmsse: 0.598146\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's wrmsse: 0.546886\tvalid_1's wrmsse: 0.531053\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.705079\tvalid_1's wrmsse: 0.674593\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's wrmsse: 0.638397\tvalid_1's wrmsse: 0.610984\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.668766\tvalid_1's wrmsse: 0.574058\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's wrmsse: 0.700617\tvalid_1's wrmsse: 0.569066\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.6573\tvalid_1's wrmsse: 0.733608\n",
      "[200]\ttraining's wrmsse: 0.632854\tvalid_1's wrmsse: 0.730581\n",
      "[300]\ttraining's wrmsse: 0.61959\tvalid_1's wrmsse: 0.730412\n",
      "[400]\ttraining's wrmsse: 0.603345\tvalid_1's wrmsse: 0.73054\n",
      "Early stopping, best iteration is:\n",
      "[319]\ttraining's wrmsse: 0.615605\tvalid_1's wrmsse: 0.728385\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.55113\tvalid_1's wrmsse: 0.509028\n",
      "[200]\ttraining's wrmsse: 0.537459\tvalid_1's wrmsse: 0.507489\n",
      "[300]\ttraining's wrmsse: 0.52618\tvalid_1's wrmsse: 0.505079\n",
      "[400]\ttraining's wrmsse: 0.511841\tvalid_1's wrmsse: 0.501907\n",
      "[500]\ttraining's wrmsse: 0.500175\tvalid_1's wrmsse: 0.499492\n",
      "[600]\ttraining's wrmsse: 0.491281\tvalid_1's wrmsse: 0.498605\n",
      "[700]\ttraining's wrmsse: 0.484397\tvalid_1's wrmsse: 0.502013\n",
      "Early stopping, best iteration is:\n",
      "[652]\ttraining's wrmsse: 0.487342\tvalid_1's wrmsse: 0.497797\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day28-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's wrmsse: 0.52982\tvalid_1's wrmsse: 0.497912\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's wrmsse: 0.563574\tvalid_1's wrmsse: 0.469148\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.953091\tvalid_1's wrmsse: 0.616587\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's wrmsse: 1.01535\tvalid_1's wrmsse: 0.574275\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.556936\tvalid_1's wrmsse: 0.703681\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's wrmsse: 0.55493\tvalid_1's wrmsse: 0.636365\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.611115\tvalid_1's wrmsse: 0.63561\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's wrmsse: 0.63136\tvalid_1's wrmsse: 0.624054\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.56856\tvalid_1's wrmsse: 0.561002\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's wrmsse: 0.571239\tvalid_1's wrmsse: 0.541094\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.542289\tvalid_1's wrmsse: 0.628931\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's wrmsse: 0.55877\tvalid_1's wrmsse: 0.540512\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.704046\tvalid_1's wrmsse: 0.676988\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's wrmsse: 0.630458\tvalid_1's wrmsse: 0.615787\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.665394\tvalid_1's wrmsse: 0.57253\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's wrmsse: 0.72573\tvalid_1's wrmsse: 0.558769\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.632879\tvalid_1's wrmsse: 0.755792\n",
      "[200]\ttraining's wrmsse: 0.611184\tvalid_1's wrmsse: 0.750668\n",
      "[300]\ttraining's wrmsse: 0.594052\tvalid_1's wrmsse: 0.743695\n",
      "[400]\ttraining's wrmsse: 0.584214\tvalid_1's wrmsse: 0.743149\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttraining's wrmsse: 0.587402\tvalid_1's wrmsse: 0.740512\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.558999\tvalid_1's wrmsse: 0.515614\n",
      "[200]\ttraining's wrmsse: 0.542063\tvalid_1's wrmsse: 0.50902\n",
      "[300]\ttraining's wrmsse: 0.528911\tvalid_1's wrmsse: 0.507475\n",
      "[400]\ttraining's wrmsse: 0.514698\tvalid_1's wrmsse: 0.502917\n",
      "[500]\ttraining's wrmsse: 0.50682\tvalid_1's wrmsse: 0.501393\n",
      "Early stopping, best iteration is:\n",
      "[451]\ttraining's wrmsse: 0.509538\tvalid_1's wrmsse: 0.50055\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day1-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.520914\tvalid_1's wrmsse: 0.547597\n",
      "[200]\ttraining's wrmsse: 0.479228\tvalid_1's wrmsse: 0.532413\n",
      "[300]\ttraining's wrmsse: 0.455705\tvalid_1's wrmsse: 0.524229\n",
      "Early stopping, best iteration is:\n",
      "[295]\ttraining's wrmsse: 0.456115\tvalid_1's wrmsse: 0.523841\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.598439\tvalid_1's wrmsse: 1.07115\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's wrmsse: 0.660934\tvalid_1's wrmsse: 1.0512\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.61063\tvalid_1's wrmsse: 0.527178\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's wrmsse: 0.600454\tvalid_1's wrmsse: 0.524925\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.673502\tvalid_1's wrmsse: 0.627027\n",
      "[200]\ttraining's wrmsse: 0.650524\tvalid_1's wrmsse: 0.605983\n",
      "[300]\ttraining's wrmsse: 0.634176\tvalid_1's wrmsse: 0.597608\n",
      "[400]\ttraining's wrmsse: 0.621266\tvalid_1's wrmsse: 0.593603\n",
      "[500]\ttraining's wrmsse: 0.605248\tvalid_1's wrmsse: 0.589078\n",
      "[600]\ttraining's wrmsse: 0.597204\tvalid_1's wrmsse: 0.588385\n",
      "Early stopping, best iteration is:\n",
      "[561]\ttraining's wrmsse: 0.59906\tvalid_1's wrmsse: 0.587243\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.456406\tvalid_1's wrmsse: 0.558239\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's wrmsse: 0.454206\tvalid_1's wrmsse: 0.547374\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.510063\tvalid_1's wrmsse: 0.541866\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's wrmsse: 0.546148\tvalid_1's wrmsse: 0.521283\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.558719\tvalid_1's wrmsse: 0.65868\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's wrmsse: 0.618971\tvalid_1's wrmsse: 0.597009\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.525383\tvalid_1's wrmsse: 0.712956\n",
      "[200]\ttraining's wrmsse: 0.500103\tvalid_1's wrmsse: 0.690031\n",
      "[300]\ttraining's wrmsse: 0.481292\tvalid_1's wrmsse: 0.685071\n",
      "[400]\ttraining's wrmsse: 0.469553\tvalid_1's wrmsse: 0.681513\n",
      "[500]\ttraining's wrmsse: 0.4611\tvalid_1's wrmsse: 0.681565\n",
      "Early stopping, best iteration is:\n",
      "[434]\ttraining's wrmsse: 0.467521\tvalid_1's wrmsse: 0.679365\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.667148\tvalid_1's wrmsse: 0.666717\n",
      "Early stopping, best iteration is:\n",
      "[95]\ttraining's wrmsse: 0.669409\tvalid_1's wrmsse: 0.663418\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.577899\tvalid_1's wrmsse: 0.560456\n",
      "[200]\ttraining's wrmsse: 0.559464\tvalid_1's wrmsse: 0.559228\n",
      "[300]\ttraining's wrmsse: 0.547775\tvalid_1's wrmsse: 0.557661\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttraining's wrmsse: 0.551262\tvalid_1's wrmsse: 0.556267\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day7-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.517724\tvalid_1's wrmsse: 0.572653\n",
      "[200]\ttraining's wrmsse: 0.484159\tvalid_1's wrmsse: 0.553554\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's wrmsse: 0.486446\tvalid_1's wrmsse: 0.552119\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.587142\tvalid_1's wrmsse: 1.1634\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's wrmsse: 0.703598\tvalid_1's wrmsse: 1.11452\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.631909\tvalid_1's wrmsse: 0.582206\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.602619\tvalid_1's wrmsse: 0.574946\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.681422\tvalid_1's wrmsse: 0.643033\n",
      "[200]\ttraining's wrmsse: 0.663627\tvalid_1's wrmsse: 0.629071\n",
      "[300]\ttraining's wrmsse: 0.647292\tvalid_1's wrmsse: 0.623\n",
      "[400]\ttraining's wrmsse: 0.635386\tvalid_1's wrmsse: 0.620246\n",
      "[500]\ttraining's wrmsse: 0.621123\tvalid_1's wrmsse: 0.613547\n",
      "[600]\ttraining's wrmsse: 0.612246\tvalid_1's wrmsse: 0.611851\n",
      "[700]\ttraining's wrmsse: 0.594599\tvalid_1's wrmsse: 0.612002\n",
      "Early stopping, best iteration is:\n",
      "[696]\ttraining's wrmsse: 0.598923\tvalid_1's wrmsse: 0.611407\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.462209\tvalid_1's wrmsse: 0.583567\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.471657\tvalid_1's wrmsse: 0.575903\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.515321\tvalid_1's wrmsse: 0.563815\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's wrmsse: 0.571862\tvalid_1's wrmsse: 0.526586\n",
      "START_TX_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.570967\tvalid_1's wrmsse: 0.688125\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's wrmsse: 0.648226\tvalid_1's wrmsse: 0.619513\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.538539\tvalid_1's wrmsse: 0.734976\n",
      "[200]\ttraining's wrmsse: 0.517359\tvalid_1's wrmsse: 0.720127\n",
      "[300]\ttraining's wrmsse: 0.50407\tvalid_1's wrmsse: 0.716037\n",
      "[400]\ttraining's wrmsse: 0.488115\tvalid_1's wrmsse: 0.710098\n",
      "[500]\ttraining's wrmsse: 0.481184\tvalid_1's wrmsse: 0.711047\n",
      "Early stopping, best iteration is:\n",
      "[411]\ttraining's wrmsse: 0.487453\tvalid_1's wrmsse: 0.708864\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.712515\tvalid_1's wrmsse: 0.715353\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.800134\tvalid_1's wrmsse: 0.702052\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.583532\tvalid_1's wrmsse: 0.598134\n",
      "[200]\ttraining's wrmsse: 0.564008\tvalid_1's wrmsse: 0.589643\n",
      "[300]\ttraining's wrmsse: 0.551137\tvalid_1's wrmsse: 0.587464\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's wrmsse: 0.553809\tvalid_1's wrmsse: 0.583839\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day14-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.518882\tvalid_1's wrmsse: 0.574451\n",
      "[200]\ttraining's wrmsse: 0.476864\tvalid_1's wrmsse: 0.553267\n",
      "[300]\ttraining's wrmsse: 0.460524\tvalid_1's wrmsse: 0.55796\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's wrmsse: 0.475049\tvalid_1's wrmsse: 0.551262\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.595538\tvalid_1's wrmsse: 1.13304\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's wrmsse: 0.719829\tvalid_1's wrmsse: 1.07002\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.616224\tvalid_1's wrmsse: 0.623613\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's wrmsse: 0.591692\tvalid_1's wrmsse: 0.579186\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.684522\tvalid_1's wrmsse: 0.634821\n",
      "[200]\ttraining's wrmsse: 0.660005\tvalid_1's wrmsse: 0.619234\n",
      "[300]\ttraining's wrmsse: 0.646604\tvalid_1's wrmsse: 0.613305\n",
      "[400]\ttraining's wrmsse: 0.632664\tvalid_1's wrmsse: 0.609137\n",
      "[500]\ttraining's wrmsse: 0.618719\tvalid_1's wrmsse: 0.606392\n",
      "[600]\ttraining's wrmsse: 0.608918\tvalid_1's wrmsse: 0.604599\n",
      "[700]\ttraining's wrmsse: 0.592197\tvalid_1's wrmsse: 0.601568\n",
      "[800]\ttraining's wrmsse: 0.584735\tvalid_1's wrmsse: 0.598348\n",
      "Early stopping, best iteration is:\n",
      "[793]\ttraining's wrmsse: 0.586851\tvalid_1's wrmsse: 0.597998\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.480918\tvalid_1's wrmsse: 0.610703\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.498138\tvalid_1's wrmsse: 0.596831\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.533327\tvalid_1's wrmsse: 0.590899\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's wrmsse: 0.617274\tvalid_1's wrmsse: 0.546565\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.597892\tvalid_1's wrmsse: 0.748128\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's wrmsse: 0.703138\tvalid_1's wrmsse: 0.635154\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.541814\tvalid_1's wrmsse: 0.737537\n",
      "[200]\ttraining's wrmsse: 0.520717\tvalid_1's wrmsse: 0.727781\n",
      "[300]\ttraining's wrmsse: 0.507097\tvalid_1's wrmsse: 0.715777\n",
      "[400]\ttraining's wrmsse: 0.493199\tvalid_1's wrmsse: 0.709061\n",
      "Early stopping, best iteration is:\n",
      "[375]\ttraining's wrmsse: 0.493354\tvalid_1's wrmsse: 0.706076\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.733672\tvalid_1's wrmsse: 0.742631\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.859994\tvalid_1's wrmsse: 0.722829\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.610257\tvalid_1's wrmsse: 0.599929\n",
      "[200]\ttraining's wrmsse: 0.589831\tvalid_1's wrmsse: 0.596664\n",
      "[300]\ttraining's wrmsse: 0.577021\tvalid_1's wrmsse: 0.593297\n",
      "[400]\ttraining's wrmsse: 0.565279\tvalid_1's wrmsse: 0.590711\n",
      "[500]\ttraining's wrmsse: 0.556735\tvalid_1's wrmsse: 0.588088\n",
      "[600]\ttraining's wrmsse: 0.550338\tvalid_1's wrmsse: 0.588165\n",
      "Early stopping, best iteration is:\n",
      "[513]\ttraining's wrmsse: 0.555071\tvalid_1's wrmsse: 0.586653\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day21-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.522701\tvalid_1's wrmsse: 0.597721\n",
      "[200]\ttraining's wrmsse: 0.486037\tvalid_1's wrmsse: 0.57186\n",
      "[300]\ttraining's wrmsse: 0.46302\tvalid_1's wrmsse: 0.569159\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttraining's wrmsse: 0.470958\tvalid_1's wrmsse: 0.564441\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.596774\tvalid_1's wrmsse: 1.13807\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's wrmsse: 0.695647\tvalid_1's wrmsse: 1.07766\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.610083\tvalid_1's wrmsse: 0.613428\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's wrmsse: 0.588497\tvalid_1's wrmsse: 0.575368\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.683751\tvalid_1's wrmsse: 0.641661\n",
      "[200]\ttraining's wrmsse: 0.653286\tvalid_1's wrmsse: 0.621766\n",
      "[300]\ttraining's wrmsse: 0.639879\tvalid_1's wrmsse: 0.620127\n",
      "[400]\ttraining's wrmsse: 0.623991\tvalid_1's wrmsse: 0.615027\n",
      "[500]\ttraining's wrmsse: 0.612452\tvalid_1's wrmsse: 0.613911\n",
      "[600]\ttraining's wrmsse: 0.601478\tvalid_1's wrmsse: 0.611624\n",
      "Early stopping, best iteration is:\n",
      "[598]\ttraining's wrmsse: 0.60233\tvalid_1's wrmsse: 0.611386\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.490586\tvalid_1's wrmsse: 0.609911\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's wrmsse: 0.506288\tvalid_1's wrmsse: 0.596648\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.535932\tvalid_1's wrmsse: 0.600856\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's wrmsse: 0.629953\tvalid_1's wrmsse: 0.553217\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.594233\tvalid_1's wrmsse: 0.779666\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's wrmsse: 0.702067\tvalid_1's wrmsse: 0.639859\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.540287\tvalid_1's wrmsse: 0.734096\n",
      "[200]\ttraining's wrmsse: 0.519457\tvalid_1's wrmsse: 0.721966\n",
      "[300]\ttraining's wrmsse: 0.502302\tvalid_1's wrmsse: 0.710449\n",
      "[400]\ttraining's wrmsse: 0.488253\tvalid_1's wrmsse: 0.703457\n",
      "Early stopping, best iteration is:\n",
      "[370]\ttraining's wrmsse: 0.491884\tvalid_1's wrmsse: 0.701368\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.761969\tvalid_1's wrmsse: 0.720796\n",
      "[200]\ttraining's wrmsse: 0.721946\tvalid_1's wrmsse: 0.723955\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's wrmsse: 0.759894\tvalid_1's wrmsse: 0.71822\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.607908\tvalid_1's wrmsse: 0.592624\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's wrmsse: 0.634464\tvalid_1's wrmsse: 0.586902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n",
      "-----day28-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.514263\tvalid_1's wrmsse: 0.579352\n",
      "[200]\ttraining's wrmsse: 0.474159\tvalid_1's wrmsse: 0.554371\n",
      "[300]\ttraining's wrmsse: 0.461379\tvalid_1's wrmsse: 0.550571\n",
      "[400]\ttraining's wrmsse: 0.446699\tvalid_1's wrmsse: 0.547432\n",
      "[500]\ttraining's wrmsse: 0.441194\tvalid_1's wrmsse: 0.550332\n",
      "[600]\ttraining's wrmsse: 0.431121\tvalid_1's wrmsse: 0.54725\n",
      "[700]\ttraining's wrmsse: 0.422755\tvalid_1's wrmsse: 0.546231\n",
      "Early stopping, best iteration is:\n",
      "[682]\ttraining's wrmsse: 0.424602\tvalid_1's wrmsse: 0.541856\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.601579\tvalid_1's wrmsse: 1.15924\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's wrmsse: 0.721861\tvalid_1's wrmsse: 1.0857\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.614974\tvalid_1's wrmsse: 0.621176\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's wrmsse: 0.596036\tvalid_1's wrmsse: 0.574306\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.681536\tvalid_1's wrmsse: 0.650595\n",
      "[200]\ttraining's wrmsse: 0.674089\tvalid_1's wrmsse: 0.629576\n",
      "[300]\ttraining's wrmsse: 0.652249\tvalid_1's wrmsse: 0.616824\n",
      "[400]\ttraining's wrmsse: 0.640844\tvalid_1's wrmsse: 0.618583\n",
      "Early stopping, best iteration is:\n",
      "[322]\ttraining's wrmsse: 0.651099\tvalid_1's wrmsse: 0.616668\n",
      "START_TX_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.474741\tvalid_1's wrmsse: 0.602917\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's wrmsse: 0.480917\tvalid_1's wrmsse: 0.591631\n",
      "START_TX_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.530656\tvalid_1's wrmsse: 0.582396\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's wrmsse: 0.603156\tvalid_1's wrmsse: 0.553289\n",
      "START_TX_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.590279\tvalid_1's wrmsse: 0.78978\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's wrmsse: 0.725831\tvalid_1's wrmsse: 0.641611\n",
      "START_WI_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.542721\tvalid_1's wrmsse: 0.739057\n",
      "[200]\ttraining's wrmsse: 0.521395\tvalid_1's wrmsse: 0.722613\n",
      "[300]\ttraining's wrmsse: 0.508183\tvalid_1's wrmsse: 0.713402\n",
      "[400]\ttraining's wrmsse: 0.495635\tvalid_1's wrmsse: 0.706007\n",
      "[500]\ttraining's wrmsse: 0.487902\tvalid_1's wrmsse: 0.705161\n",
      "[600]\ttraining's wrmsse: 0.477278\tvalid_1's wrmsse: 0.702756\n",
      "Early stopping, best iteration is:\n",
      "[564]\ttraining's wrmsse: 0.481672\tvalid_1's wrmsse: 0.701667\n",
      "START_WI_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.739984\tvalid_1's wrmsse: 0.69911\n",
      "[200]\ttraining's wrmsse: 0.70399\tvalid_1's wrmsse: 0.699589\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's wrmsse: 0.737511\tvalid_1's wrmsse: 0.696569\n",
      "START_WI_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's wrmsse: 0.587197\tvalid_1's wrmsse: 0.600381\n",
      "[200]\ttraining's wrmsse: 0.572733\tvalid_1's wrmsse: 0.598477\n",
      "[300]\ttraining's wrmsse: 0.563297\tvalid_1's wrmsse: 0.595946\n",
      "[400]\ttraining's wrmsse: 0.555219\tvalid_1's wrmsse: 0.598349\n",
      "Early stopping, best iteration is:\n",
      "[310]\ttraining's wrmsse: 0.562094\tvalid_1's wrmsse: 0.595249\n",
      "predict\n",
      "CA_1\n",
      "CA_2\n",
      "CA_3\n",
      "CA_4\n",
      "TX_1\n",
      "TX_2\n",
      "TX_3\n",
      "WI_1\n",
      "WI_2\n",
      "WI_3\n"
     ]
    }
   ],
   "source": [
    "#### MAKE DATASET & Train Models\n",
    "#################################################################################\n",
    "\n",
    "# df = df[df[\"non_sales_for_2month\"] == 0]\n",
    "# df = df[df.sell_price >= 0]\n",
    "\n",
    "\n",
    "for term in [\"public\", \"validation\", \"semival\"]:\n",
    "    for SHIFT_DAY in [1, 7, 14, 21, 28]:\n",
    "        print(f\"-----day{SHIFT_DAY}-------------\")\n",
    "        \n",
    "    \n",
    "        if term == \"private\":\n",
    "            TRAIN_END_DATE = \"2016-05-22\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-05-22\"\n",
    "            VAL_START_DATE = \"2016-04-25\"\n",
    "            EVAL_END_DATE = \"2016-06-19\"\n",
    "            EVAL_START_DATE = \"2016-05-23\"\n",
    "            DAYS_COEF = 0    \n",
    "            \n",
    "        elif term == \"public\":\n",
    "            TRAIN_END_DATE = \"2016-04-24\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-05-22\"\n",
    "            VAL_START_DATE = \"2016-04-25\"\n",
    "            EVAL_END_DATE = \"2016-05-22\"\n",
    "            EVAL_START_DATE = \"2016-04-25\"\n",
    "            DAYS_COEF = 1\n",
    "            \n",
    "        elif term == \"validation\":\n",
    "            TRAIN_END_DATE = \"2016-03-27\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-04-24\"\n",
    "            VAL_START_DATE = \"2016-03-28\"\n",
    "            EVAL_END_DATE = \"2016-04-24\"\n",
    "            EVAL_START_DATE = \"2016-03-28\"\n",
    "            DAYS_COEF = 2\n",
    "\n",
    "        elif term == \"semival\":\n",
    "            TRAIN_END_DATE = \"2016-02-28\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-03-27\"\n",
    "            VAL_START_DATE = \"2016-02-29\"\n",
    "            EVAL_END_DATE = \"2016-03-27\"\n",
    "            EVAL_START_DATE = \"2016-02-29\"\n",
    "            DAYS_COEF = 3\n",
    "        \n",
    "        END_TRAIN = 1941-28*DAYS_COEF\n",
    "        \n",
    "        dt1 = datetime.strptime(TRAIN_START_DATE,'%Y-%m-%d')\n",
    "        dt2 = datetime.strptime(TRAIN_END_DATE,'%Y-%m-%d')\n",
    "        dt = dt2 - dt1\n",
    "        TRAIN_DAYS = dt.days + 1\n",
    "\n",
    "        day_from = 1\n",
    "        day_to = 28\n",
    "\n",
    "        print(\"load data\")\n",
    "        base_df = pd.read_pickle(f\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/data_base_{term}_df.pkl\") #f\"DataFrame/data_base_{term}_df.pkl\")\n",
    "        lag_df = pd.read_pickle(f\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/data_day{SHIFT_DAY}_df.pkl\") #f\"DataFrame/data_day{SHIFT_DAY}_df.pkl\")\n",
    "        df = pd.concat((base_df, lag_df), axis=1)\n",
    "\n",
    "        del base_df, lag_df\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"category feat\")\n",
    "        for col in CAT_FEATURES:\n",
    "            try:\n",
    "                df[col] = df[col].astype('category')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        train_df = pd.read_csv(\"m5-forecasting-accuracy/sales_train_evaluation.csv\")\n",
    "        category_id_origin = train_df[f\"{category_name}_id\"]  ####\n",
    "\n",
    "        del train_df,\n",
    "        gc.collect()\n",
    "        \n",
    "        WEIGHT_SCALED_30490 = np.load(\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/submit_data/Accuracy/weights/WEIGHT_SCALED_30490.npy\")\n",
    "\n",
    "        for category in CATEGORY_ID:\n",
    "            print(f\"START_{category}\")\n",
    "            category_param(category)\n",
    "            df_category = df[df[f\"{category_name}_id\"]==category] ####\n",
    "\n",
    "            category_mask = category_id_origin == category\n",
    "\n",
    "            weight_category = WEIGHT_SCALED_30490[category_mask]\n",
    "\n",
    "            NUM_ITEMS = len(weight_category)\n",
    "\n",
    "            # lgb_w_tr, lgb_w_val, lgb_w_eval = train_weight(df,)\n",
    "            lgb_w_tr = np.tile(weight_category, TRAIN_DAYS)\n",
    "            tr_x, tr_y, val_x, val_y, eval_x, eval_x_id_date = data_division(df_category,)\n",
    "\n",
    "            weight1 = np.load(f\"weights/weight1_{category}.npy\")\n",
    "            weight2 = np.load(f\"weights/weight2_{category}.npy\")\n",
    "            weight_mat = np.load(f\"weights/weight_mat_{category}.npy\")\n",
    "            weight_mat_csr = csr_matrix(weight_mat)\n",
    "\n",
    "            # eval_x = eval_x.drop(DROP_FEATURES_ROLL, axis=1)\n",
    "            # eval_x = eval_x.drop(DROP_FEATURES_SHIFT, axis=1)\n",
    "\n",
    "            MODEL_FEATURES = tr_x.columns\n",
    "\n",
    "            del df_category,\n",
    "            gc.collect()\n",
    "\n",
    "            #Dataset作成\n",
    "            train_set = lgb.Dataset(tr_x, tr_y, weight=lgb_w_tr) #categorical_feature = CAT_FEATURES)  # weight=lgb_w_tr, \n",
    "            val_set = lgb.Dataset(val_x, val_y,) #categorical_feature = CAT_FEATURES)\n",
    "\n",
    "            del tr_x, tr_y, #lgb_w_tr, lgb_w_val, lgb_w_eval\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            ########################### Train\n",
    "            #################################################################################\n",
    "\n",
    "            SAVE_MODEL_PATH = f'model/model_{term}_day{SHIFT_DAY}_{category}.pkl'\n",
    "\n",
    "            # with open('/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/model/model_87_stats28_roll28_4year_WRMSSE_lag_shift1_2_7.pkl', 'rb') as fin:\n",
    "            #     init_model = pickle.load(fin)\n",
    "\n",
    "            model = lgb.train(params, \n",
    "                              train_set, \n",
    "            #                   num_boost_round = 10000, \n",
    "                              early_stopping_rounds=100, \n",
    "                              valid_sets = [train_set, val_set], \n",
    "                              verbose_eval = 100,\n",
    "            #                   init_model=init_model,\n",
    "        #                       fobj=obj_wrmsse4,\n",
    "                              feval=wrmsse,\n",
    "            #                   callbacks=[lgb.reset_parameter(learning_rate=decay_learning_rate)]\n",
    "                             )\n",
    "\n",
    "            with open(SAVE_MODEL_PATH, 'wb') as fout:\n",
    "                pickle.dump(model, fout)\n",
    "\n",
    "\n",
    "#             fi = pd.DataFrame(model.feature_importance(importance_type='gain'), index=MODEL_FEATURES, columns=[\"importances\"])\n",
    "#             graph_feature_importance(fi)\n",
    "\n",
    "\n",
    "#         private_pred = pd.read_pickle(\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/pred/private_pred_2.pkl\")\n",
    "        private_pred = pd.read_pickle(f\"prediction/{term}_pred_shift{SHIFT_DAY}.pkl\")\n",
    "\n",
    "        PRED_DAY_TERM = [day_from, day_to]\n",
    "\n",
    "        eval_x = df[(df['date'] >= EVAL_START_DATE) & (df[\"date\"] <= EVAL_END_DATE)]\n",
    "\n",
    "        test_df = eval_x.copy()\n",
    "\n",
    "        # day_mask = (sales_df['d_serial']>=(END_TRAIN+PRED_DAY_TERM[0]))&(sales_df['d_serial']<=(END_TRAIN+PRED_DAY_TERM[1]))\n",
    "\n",
    "        print(\"predict\")\n",
    "\n",
    "        for category in CATEGORY_ID:\n",
    "            with open(f'model/model_{term}_day{SHIFT_DAY}_{category}.pkl', 'rb') as fin:\n",
    "                model = pickle.load(fin)\n",
    "\n",
    "            day_mask = (private_pred[f\"{category_name}_id\"]==category)&(private_pred.d_serial>=(END_TRAIN+PRED_DAY_TERM[0]))&(private_pred.d_serial<=(END_TRAIN+PRED_DAY_TERM[1]))\n",
    "\n",
    "            private_pred.loc[(private_pred[f\"{category_name}_id\"]==category)&(private_pred.d_serial>=(END_TRAIN+PRED_DAY_TERM[0]))&(private_pred.d_serial<=(END_TRAIN+PRED_DAY_TERM[1])),\"pred\"] = model.predict(test_df.loc[day_mask, MODEL_FEATURES])\n",
    "\n",
    "            print(category)\n",
    "\n",
    "#         private_pred.to_pickle(\"/Users/hiroaki_ikeshita/myfolder/diveintocode-ml/Kaggle/Walmart/DataFrame/pred/private_pred_2.pkl\")\n",
    "        private_pred.to_pickle(f\"prediction/{term}_pred_shift{SHIFT_DAY}.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN PRIVATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T14:37:20.769317Z",
     "start_time": "2020-07-04T14:37:20.760332Z"
    }
   },
   "outputs": [],
   "source": [
    "##################### Model Params\n",
    "####################################################################\n",
    "\n",
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': \"regression\",\n",
    "#             'alpha':0.5,\n",
    "#             'tweedie_variance_power': 1.1,\n",
    "#             \"force_row_wise\" : True,\n",
    "#             'max_depth':11,\n",
    "            'metric': 'custom',\n",
    "#             'metric': '[rmse]',\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.01,\n",
    "#             'learning_rate': 0.08,\n",
    "            'num_leaves': 2**11-1,   \n",
    "            'min_data_in_leaf': 2**12-1,\n",
    "#             \"lambda_l2\" : 0.1,\n",
    "            'feature_fraction': 0.8,\n",
    "            'max_bin': 255,     \n",
    "            'n_estimators': 1500,\n",
    "            'boost_from_average': False,\n",
    "            'verbose': -1,\n",
    "#             \"is_enable_sparse\": False,\n",
    "              }\n",
    "def default_params():\n",
    "        params[\"subsample\"] = 0.5\n",
    "        params[\"learning_rate\"] = 0.01\n",
    "#         params[\"learning_rate\"] = 0.08\n",
    "        params[\"num_leaves\"] = 2**11-1\n",
    "        params[\"min_data_in_leaf\"] = 2**12-1\n",
    "        params[\"feature_fraction\"] = 0.8\n",
    "        params[\"max_bin\"] = 255\n",
    "        params[\"n_estimators\"] =1500\n",
    "        \n",
    "def category_param(category):\n",
    "    default_params()\n",
    "    if category==\"CA_1\":\n",
    "#         params[\"n_estimators\"] =1200\n",
    "        pass\n",
    "\n",
    "    elif category==\"CA_2\":\n",
    "        params[\"num_leaves\"] = 2**8-1\n",
    "        params[\"min_data_in_leaf\"] = 2**8-1\n",
    "        \n",
    "    elif category==\"CA_3\":\n",
    "#         params[\"learning_rate\"] = 0.03\n",
    "        params[\"num_leaves\"] = 2**8-1\n",
    "        params[\"min_data_in_leaf\"] = 2**8-1\n",
    "        params[\"n_estimators\"] =2300\n",
    "        \n",
    "    elif category==\"CA_4\":\n",
    "        params[\"feature_fraction\"] = 0.5\n",
    "        \n",
    "    elif category==\"TX_1\":\n",
    "        pass\n",
    "    elif category==\"TX_2\":\n",
    "        pass\n",
    "    elif category==\"TX_3\":\n",
    "        pass\n",
    "    elif category==\"WI_1\":\n",
    "        pass\n",
    "    elif category==\"WI_2\":\n",
    "        params[\"num_leaves\"] = 2**8-1\n",
    "        params[\"min_data_in_leaf\"] = 2**8-1\n",
    "        params[\"feature_fraction\"] = 0.5\n",
    "    elif category==\"WI_3\":\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T16:01:36.850680Z",
     "start_time": "2020-07-04T15:01:19.728476Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----day1-------------\n",
      "load data\n",
      "category feat\n",
      "START_CA_1\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "[100]\ttraining's wrmsse: 0.550035\tvalid_1's wrmsse: 0.550035\n",
      "[200]\ttraining's wrmsse: 0.524838\tvalid_1's wrmsse: 0.524838\n",
      "[300]\ttraining's wrmsse: 0.502691\tvalid_1's wrmsse: 0.502691\n",
      "[400]\ttraining's wrmsse: 0.49092\tvalid_1's wrmsse: 0.49092\n",
      "[500]\ttraining's wrmsse: 0.481347\tvalid_1's wrmsse: 0.481347\n",
      "[600]\ttraining's wrmsse: 0.477659\tvalid_1's wrmsse: 0.477659\n",
      "[700]\ttraining's wrmsse: 0.468626\tvalid_1's wrmsse: 0.468626\n",
      "[800]\ttraining's wrmsse: 0.46326\tvalid_1's wrmsse: 0.46326\n",
      "[900]\ttraining's wrmsse: 0.460429\tvalid_1's wrmsse: 0.460429\n",
      "[1000]\ttraining's wrmsse: 0.451937\tvalid_1's wrmsse: 0.451937\n",
      "[1100]\ttraining's wrmsse: 0.450743\tvalid_1's wrmsse: 0.450743\n",
      "[1200]\ttraining's wrmsse: 0.450867\tvalid_1's wrmsse: 0.450867\n",
      "[1300]\ttraining's wrmsse: 0.449172\tvalid_1's wrmsse: 0.449172\n",
      "[1400]\ttraining's wrmsse: 0.448922\tvalid_1's wrmsse: 0.448922\n",
      "[1500]\ttraining's wrmsse: 0.443654\tvalid_1's wrmsse: 0.443654\n",
      "START_CA_2\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "[100]\ttraining's wrmsse: 0.597968\tvalid_1's wrmsse: 0.597968\n",
      "[200]\ttraining's wrmsse: 0.562854\tvalid_1's wrmsse: 0.562854\n",
      "[300]\ttraining's wrmsse: 0.528129\tvalid_1's wrmsse: 0.528129\n",
      "[400]\ttraining's wrmsse: 0.521365\tvalid_1's wrmsse: 0.521365\n",
      "[500]\ttraining's wrmsse: 0.506064\tvalid_1's wrmsse: 0.506064\n",
      "[600]\ttraining's wrmsse: 0.504964\tvalid_1's wrmsse: 0.504964\n",
      "[700]\ttraining's wrmsse: 0.492197\tvalid_1's wrmsse: 0.492197\n",
      "[800]\ttraining's wrmsse: 0.482515\tvalid_1's wrmsse: 0.482515\n",
      "[900]\ttraining's wrmsse: 0.475845\tvalid_1's wrmsse: 0.475845\n",
      "[1000]\ttraining's wrmsse: 0.469166\tvalid_1's wrmsse: 0.469166\n",
      "[1100]\ttraining's wrmsse: 0.463304\tvalid_1's wrmsse: 0.463304\n",
      "[1200]\ttraining's wrmsse: 0.451909\tvalid_1's wrmsse: 0.451909\n",
      "[1300]\ttraining's wrmsse: 0.44477\tvalid_1's wrmsse: 0.44477\n",
      "[1400]\ttraining's wrmsse: 0.448871\tvalid_1's wrmsse: 0.448871\n",
      "[1500]\ttraining's wrmsse: 0.441736\tvalid_1's wrmsse: 0.441736\n",
      "START_CA_3\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "[100]\ttraining's wrmsse: 0.755661\tvalid_1's wrmsse: 0.755661\n",
      "[200]\ttraining's wrmsse: 0.659499\tvalid_1's wrmsse: 0.659499\n",
      "[300]\ttraining's wrmsse: 0.60202\tvalid_1's wrmsse: 0.60202\n",
      "[400]\ttraining's wrmsse: 0.585607\tvalid_1's wrmsse: 0.585607\n",
      "[500]\ttraining's wrmsse: 0.55288\tvalid_1's wrmsse: 0.55288\n",
      "[600]\ttraining's wrmsse: 0.526936\tvalid_1's wrmsse: 0.526936\n",
      "[700]\ttraining's wrmsse: 0.503725\tvalid_1's wrmsse: 0.503725\n",
      "[800]\ttraining's wrmsse: 0.489727\tvalid_1's wrmsse: 0.489727\n",
      "[900]\ttraining's wrmsse: 0.479585\tvalid_1's wrmsse: 0.479585\n",
      "[1000]\ttraining's wrmsse: 0.473904\tvalid_1's wrmsse: 0.473904\n",
      "[1100]\ttraining's wrmsse: 0.46299\tvalid_1's wrmsse: 0.46299\n",
      "[1200]\ttraining's wrmsse: 0.459569\tvalid_1's wrmsse: 0.459569\n",
      "[1300]\ttraining's wrmsse: 0.457932\tvalid_1's wrmsse: 0.457932\n",
      "[1400]\ttraining's wrmsse: 0.455316\tvalid_1's wrmsse: 0.455316\n",
      "[1500]\ttraining's wrmsse: 0.455419\tvalid_1's wrmsse: 0.455419\n",
      "[1600]\ttraining's wrmsse: 0.442366\tvalid_1's wrmsse: 0.442366\n",
      "[1700]\ttraining's wrmsse: 0.434925\tvalid_1's wrmsse: 0.434925\n",
      "[1800]\ttraining's wrmsse: 0.429442\tvalid_1's wrmsse: 0.429442\n",
      "[1900]\ttraining's wrmsse: 0.418686\tvalid_1's wrmsse: 0.418686\n",
      "[2000]\ttraining's wrmsse: 0.413561\tvalid_1's wrmsse: 0.413561\n",
      "[2100]\ttraining's wrmsse: 0.412834\tvalid_1's wrmsse: 0.412834\n",
      "[2200]\ttraining's wrmsse: 0.410876\tvalid_1's wrmsse: 0.410876\n",
      "[2300]\ttraining's wrmsse: 0.399652\tvalid_1's wrmsse: 0.399652\n",
      "START_CA_4\n",
      "Data分割完了\n",
      "学習データ作成完了\n",
      "[100]\ttraining's wrmsse: 0.675136\tvalid_1's wrmsse: 0.675136\n",
      "[200]\ttraining's wrmsse: 0.628272\tvalid_1's wrmsse: 0.628272\n",
      "[300]\ttraining's wrmsse: 0.600435\tvalid_1's wrmsse: 0.600435\n",
      "[400]\ttraining's wrmsse: 0.586787\tvalid_1's wrmsse: 0.586787\n",
      "[500]\ttraining's wrmsse: 0.579337\tvalid_1's wrmsse: 0.579337\n",
      "[600]\ttraining's wrmsse: 0.563387\tvalid_1's wrmsse: 0.563387\n",
      "[700]\ttraining's wrmsse: 0.550445\tvalid_1's wrmsse: 0.550445\n",
      "[800]\ttraining's wrmsse: 0.538811\tvalid_1's wrmsse: 0.538811\n",
      "[900]\ttraining's wrmsse: 0.533536\tvalid_1's wrmsse: 0.533536\n",
      "[1000]\ttraining's wrmsse: 0.521965\tvalid_1's wrmsse: 0.521965\n",
      "[1100]\ttraining's wrmsse: 0.514246\tvalid_1's wrmsse: 0.514246\n",
      "[1200]\ttraining's wrmsse: 0.51077\tvalid_1's wrmsse: 0.51077\n",
      "[1300]\ttraining's wrmsse: 0.50899\tvalid_1's wrmsse: 0.50899\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8f85bf03e565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m#                   init_model=init_model,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m#                       fobj=obj_wrmsse4,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                               \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrmsse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;31m#                   callbacks=[lgb.reset_parameter(learning_rate=decay_learning_rate)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                              )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### MAKE DATASET & Train Models\n",
    "#################################################################################\n",
    "\n",
    "# df = df[df[\"non_sales_for_2month\"] == 0]\n",
    "# df = df[df.sell_price >= 0]\n",
    "\n",
    "\n",
    "for term in [\"private\"]:\n",
    "    for SHIFT_DAY in range(1, 29):\n",
    "        print(f\"-----day{SHIFT_DAY}-------------\")\n",
    "        \n",
    "    \n",
    "        if term == \"private\":\n",
    "            TRAIN_END_DATE = \"2016-05-22\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-05-22\"\n",
    "            VAL_START_DATE = \"2016-04-25\"\n",
    "            EVAL_END_DATE = \"2016-06-19\"\n",
    "            EVAL_START_DATE = \"2016-05-23\"\n",
    "            DAYS_COEF = 0    \n",
    "            \n",
    "        elif term == \"public\":\n",
    "            TRAIN_END_DATE = \"2016-04-24\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-05-22\"\n",
    "            VAL_START_DATE = \"2016-04-25\"\n",
    "            EVAL_END_DATE = \"2016-05-22\"\n",
    "            EVAL_START_DATE = \"2016-04-25\"\n",
    "            DAYS_COEF = 1\n",
    "            \n",
    "        elif term == \"validation\":\n",
    "            TRAIN_END_DATE = \"2016-03-27\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-04-24\"\n",
    "            VAL_START_DATE = \"2016-03-28\"\n",
    "            EVAL_END_DATE = \"2016-04-24\"\n",
    "            EVAL_START_DATE = \"2016-03-28\"\n",
    "            DAYS_COEF = 2\n",
    "\n",
    "        elif term == \"semival\":\n",
    "            TRAIN_END_DATE = \"2016-02-28\"\n",
    "            TRAIN_START_DATE = \"2012-03-28\"\n",
    "            VAL_END_DATE = \"2016-03-27\"\n",
    "            VAL_START_DATE = \"2016-02-29\"\n",
    "            EVAL_END_DATE = \"2016-03-27\"\n",
    "            EVAL_START_DATE = \"2016-02-29\"\n",
    "            DAYS_COEF = 3\n",
    "        \n",
    "        END_TRAIN = 1941-28*DAYS_COEF\n",
    "        \n",
    "        dt1 = datetime.strptime(TRAIN_START_DATE,'%Y-%m-%d')\n",
    "        dt2 = datetime.strptime(TRAIN_END_DATE,'%Y-%m-%d')\n",
    "        dt = dt2 - dt1\n",
    "        TRAIN_DAYS = dt.days + 1\n",
    "\n",
    "        day_from = SHIFT_DAY\n",
    "        day_to = SHIFT_DAY\n",
    "\n",
    "        print(\"load data\")\n",
    "        df = pd.read_pickle(f\"dataframe/data_base_{term}_df.pkl\")\n",
    "#         lag_df = pd.read_pickle(f\"dataframe/data_day{SHIFT_DAY}_df.pkl\")\n",
    "#         df = pd.concat((base_df, lag_df), axis=1)\n",
    "\n",
    "#         del base_df, lag_df\n",
    "#         gc.collect()\n",
    "\n",
    "        print(\"category feat\")\n",
    "        for col in CAT_FEATURES:\n",
    "            try:\n",
    "                df[col] = df[col].astype('category')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        train_df = pd.read_csv(\"m5-forecasting-accuracy/sales_train_evaluation.csv\")\n",
    "        category_id_origin = train_df[f\"{category_name}_id\"]  ####\n",
    "\n",
    "        del train_df,\n",
    "        gc.collect()\n",
    "\n",
    "        for category in CATEGORY_ID:\n",
    "            print(f\"START_{category}\")\n",
    "            category_param(category)\n",
    "            df_category = df[df[f\"{category_name}_id\"]==category] ####\n",
    "\n",
    "            category_mask = category_id_origin == category\n",
    "\n",
    "            weight_category = WEIGHT_SCALED_30490[category_mask]\n",
    "\n",
    "            NUM_ITEMS = len(weight_category)\n",
    "\n",
    "            # lgb_w_tr, lgb_w_val, lgb_w_eval = train_weight(df,)\n",
    "            lgb_w_tr = np.tile(weight_category, TRAIN_DAYS)\n",
    "            tr_x, tr_y, val_x, val_y, eval_x, eval_x_id_date = data_division(df_category,)\n",
    "\n",
    "            weight1 = np.load(f\"weights/weight1_{category}.npy\")\n",
    "            weight2 = np.load(f\"weights/weight2_{category}.npy\")\n",
    "            weight_mat = np.load(f\"weights/weight_mat_{category}.npy\")\n",
    "            weight_mat_csr = csr_matrix(weight_mat)\n",
    "\n",
    "            # eval_x = eval_x.drop(DROP_FEATURES_ROLL, axis=1)\n",
    "            # eval_x = eval_x.drop(DROP_FEATURES_SHIFT, axis=1)\n",
    "\n",
    "            MODEL_FEATURES = tr_x.columns\n",
    "\n",
    "            del df_category,\n",
    "            gc.collect()\n",
    "\n",
    "            #Dataset作成\n",
    "            train_set = lgb.Dataset(tr_x, tr_y, weight=lgb_w_tr) #categorical_feature = CAT_FEATURES)  # weight=lgb_w_tr, \n",
    "            val_set = lgb.Dataset(val_x, val_y,) #categorical_feature = CAT_FEATURES)\n",
    "\n",
    "            del tr_x, tr_y, #lgb_w_tr, lgb_w_val, lgb_w_eval\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "            ########################### Train\n",
    "            #################################################################################\n",
    "\n",
    "            SAVE_MODEL_PATH = f'model/model_{term}_day{SHIFT_DAY}_{category}.pkl'\n",
    "\n",
    "            # with open('/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/model/model_87_stats28_roll28_4year_WRMSSE_lag_shift1_2_7.pkl', 'rb') as fin:\n",
    "            #     init_model = pickle.load(fin)\n",
    "\n",
    "            model = lgb.train(params, \n",
    "                              train_set, \n",
    "            #                   num_boost_round = 10000, \n",
    "#                               early_stopping_rounds=100, \n",
    "                              valid_sets = [train_set, val_set], \n",
    "                              verbose_eval = 100,\n",
    "            #                   init_model=init_model,\n",
    "        #                       fobj=obj_wrmsse4,\n",
    "                              feval=wrmsse,\n",
    "            #                   callbacks=[lgb.reset_parameter(learning_rate=decay_learning_rate)]\n",
    "                             )\n",
    "\n",
    "            with open(SAVE_MODEL_PATH, 'wb') as fout:\n",
    "                pickle.dump(model, fout)\n",
    "\n",
    "\n",
    "#             fi = pd.DataFrame(model.feature_importance(importance_type='gain'), index=MODEL_FEATURES, columns=[\"importances\"])\n",
    "#             graph_feature_importance(fi)\n",
    "\n",
    "\n",
    "        private_pred = pd.read_pickle(\"prediction/private_pred.pkl\")\n",
    "#         private_pred = pd.read_pickle(f\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/predict_pkl/store/{term}_pred.pkl\")\n",
    "\n",
    "        PRED_DAY_TERM = [day_from, day_to]\n",
    "\n",
    "        eval_x = df[(df['date'] >= EVAL_START_DATE) & (df[\"date\"] <= EVAL_END_DATE)]\n",
    "\n",
    "        test_df = eval_x.copy()\n",
    "\n",
    "        # day_mask = (sales_df['d_serial']>=(END_TRAIN+PRED_DAY_TERM[0]))&(sales_df['d_serial']<=(END_TRAIN+PRED_DAY_TERM[1]))\n",
    "\n",
    "        print(\"predict\")\n",
    "\n",
    "        for category in CATEGORY_ID:\n",
    "            with open(f'model/model_{term}_day{SHIFT_DAY}_{category}.pkl', 'rb') as fin:\n",
    "                model = pickle.load(fin)\n",
    "\n",
    "            day_mask = (private_pred[f\"{category_name}_id\"]==category)&(private_pred.d_serial>=(END_TRAIN+PRED_DAY_TERM[0]))&(private_pred.d_serial<=(END_TRAIN+PRED_DAY_TERM[1]))\n",
    "\n",
    "            private_pred.loc[(private_pred[f\"{category_name}_id\"]==category)&(private_pred.d_serial>=(END_TRAIN+PRED_DAY_TERM[0]))&(private_pred.d_serial<=(END_TRAIN+PRED_DAY_TERM[1])),\"pred\"] = model.predict(test_df.loc[day_mask, MODEL_FEATURES])\n",
    "\n",
    "            print(category)\n",
    "\n",
    "        private_pred.to_pickle(\"prediction/private_pred.pkl\")\n",
    "#         private_pred.to_pickle(f\"/Volumes/Extreme SSD/kaggle/Walmart/pkl_data/predict_pkl/store/{term}_pred_shift{SHIFT_DAY}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCURACY SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T04:13:42.294602Z",
     "start_time": "2020-07-05T04:13:39.585822Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_no = 1\n",
    "private_pred = pd.read_pickle(\"prediction/private_pred.pkl\")\n",
    "\n",
    "private_pred[\"modi\"] = private_pred[\"shift_28_rolling_365\"] + private_pred[\"pred\"]\n",
    "private_pred[\"No\"] = np.tile(range(30490), 28)\n",
    "private_pred_pivot = private_pred.pivot_table(index=[\"No\", \"id\"], columns=\"d_serial\", values=\"modi\", aggfunc=np.sum)\n",
    "private_pred_pivot = private_pred_pivot.reset_index()\n",
    "private_pred_pivot = private_pred_pivot.drop(\"No\", axis=1)\n",
    "private_pred_pivot.columns = [\"id\"] + [f\"F{i}\" for i in range(1, 29)]\n",
    "eval_pred = private_pred_pivot.copy()\n",
    "eval_pred[\"id\"] = eval_pred[\"id\"].apply(lambda x: x.replace(\"validation\", \"evaluation\"))\n",
    "submission = pd.concat((private_pred_pivot, eval_pred), axis=0)\n",
    "submission.to_csv(f'accuracy_submission/accuracy_submission_private_{submission_no}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T04:14:42.033347Z",
     "start_time": "2020-07-05T04:14:42.005063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>F17</th>\n",
       "      <th>F18</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>0.679688</td>\n",
       "      <td>0.676758</td>\n",
       "      <td>0.676758</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.698730</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>0.693359</td>\n",
       "      <td>0.695801</td>\n",
       "      <td>0.698730</td>\n",
       "      <td>0.698730</td>\n",
       "      <td>0.704102</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>0.712402</td>\n",
       "      <td>0.717773</td>\n",
       "      <td>0.720703</td>\n",
       "      <td>0.720703</td>\n",
       "      <td>0.726074</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>0.736816</td>\n",
       "      <td>0.745117</td>\n",
       "      <td>0.753418</td>\n",
       "      <td>0.753418</td>\n",
       "      <td>0.756348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.345215</td>\n",
       "      <td>0.345215</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>0.356201</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.345215</td>\n",
       "      <td>0.345215</td>\n",
       "      <td>0.345215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>0.534180</td>\n",
       "      <td>0.534180</td>\n",
       "      <td>0.537109</td>\n",
       "      <td>0.539551</td>\n",
       "      <td>0.537109</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.539551</td>\n",
       "      <td>0.539551</td>\n",
       "      <td>0.545410</td>\n",
       "      <td>0.547852</td>\n",
       "      <td>0.553223</td>\n",
       "      <td>0.553223</td>\n",
       "      <td>0.553223</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.561523</td>\n",
       "      <td>0.566895</td>\n",
       "      <td>0.566895</td>\n",
       "      <td>0.566895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.956055</td>\n",
       "      <td>1.956055</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.956055</td>\n",
       "      <td>1.961914</td>\n",
       "      <td>1.961914</td>\n",
       "      <td>1.961914</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.948242</td>\n",
       "      <td>1.945312</td>\n",
       "      <td>1.951172</td>\n",
       "      <td>1.948242</td>\n",
       "      <td>1.936523</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>1.931641</td>\n",
       "      <td>1.928711</td>\n",
       "      <td>1.925781</td>\n",
       "      <td>1.909180</td>\n",
       "      <td>1.907227</td>\n",
       "      <td>1.901367</td>\n",
       "      <td>1.901367</td>\n",
       "      <td>1.909180</td>\n",
       "      <td>1.907227</td>\n",
       "      <td>1.904297</td>\n",
       "      <td>1.909180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>1.131836</td>\n",
       "      <td>1.125977</td>\n",
       "      <td>1.128906</td>\n",
       "      <td>1.131836</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>1.128906</td>\n",
       "      <td>1.128906</td>\n",
       "      <td>1.123047</td>\n",
       "      <td>1.125977</td>\n",
       "      <td>1.128906</td>\n",
       "      <td>1.131836</td>\n",
       "      <td>1.139648</td>\n",
       "      <td>1.145508</td>\n",
       "      <td>1.153320</td>\n",
       "      <td>1.159180</td>\n",
       "      <td>1.164062</td>\n",
       "      <td>1.169922</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>1.183594</td>\n",
       "      <td>1.186523</td>\n",
       "      <td>1.186523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>0.581055</td>\n",
       "      <td>0.581055</td>\n",
       "      <td>0.581055</td>\n",
       "      <td>0.586426</td>\n",
       "      <td>0.591797</td>\n",
       "      <td>0.591797</td>\n",
       "      <td>0.591797</td>\n",
       "      <td>0.591797</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.594727</td>\n",
       "      <td>0.591797</td>\n",
       "      <td>0.594727</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.613770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.030136</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.035614</td>\n",
       "      <td>0.035614</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.041107</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.046570</td>\n",
       "      <td>0.046570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.630371</td>\n",
       "      <td>0.627441</td>\n",
       "      <td>0.627441</td>\n",
       "      <td>0.624512</td>\n",
       "      <td>0.630371</td>\n",
       "      <td>0.627441</td>\n",
       "      <td>0.627441</td>\n",
       "      <td>0.619141</td>\n",
       "      <td>0.619141</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.613770</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.619141</td>\n",
       "      <td>0.622070</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.613770</td>\n",
       "      <td>0.613770</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.608398</td>\n",
       "      <td>0.602539</td>\n",
       "      <td>0.608398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>1.084961</td>\n",
       "      <td>1.092773</td>\n",
       "      <td>1.092773</td>\n",
       "      <td>1.095703</td>\n",
       "      <td>1.101562</td>\n",
       "      <td>1.104492</td>\n",
       "      <td>1.104492</td>\n",
       "      <td>1.109375</td>\n",
       "      <td>1.112305</td>\n",
       "      <td>1.115234</td>\n",
       "      <td>1.120117</td>\n",
       "      <td>1.120117</td>\n",
       "      <td>1.125977</td>\n",
       "      <td>1.128906</td>\n",
       "      <td>1.131836</td>\n",
       "      <td>1.131836</td>\n",
       "      <td>1.136719</td>\n",
       "      <td>1.139648</td>\n",
       "      <td>1.142578</td>\n",
       "      <td>1.145508</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>1.159180</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.175781</td>\n",
       "      <td>1.177734</td>\n",
       "      <td>1.180664</td>\n",
       "      <td>1.183594</td>\n",
       "      <td>1.183594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>1.723633</td>\n",
       "      <td>1.717773</td>\n",
       "      <td>1.707031</td>\n",
       "      <td>1.707031</td>\n",
       "      <td>1.707031</td>\n",
       "      <td>1.709961</td>\n",
       "      <td>1.698242</td>\n",
       "      <td>1.673828</td>\n",
       "      <td>1.666016</td>\n",
       "      <td>1.652344</td>\n",
       "      <td>1.655273</td>\n",
       "      <td>1.657227</td>\n",
       "      <td>1.655273</td>\n",
       "      <td>1.657227</td>\n",
       "      <td>1.657227</td>\n",
       "      <td>1.646484</td>\n",
       "      <td>1.640625</td>\n",
       "      <td>1.640625</td>\n",
       "      <td>1.643555</td>\n",
       "      <td>1.643555</td>\n",
       "      <td>1.643555</td>\n",
       "      <td>1.646484</td>\n",
       "      <td>1.638672</td>\n",
       "      <td>1.632812</td>\n",
       "      <td>1.635742</td>\n",
       "      <td>1.638672</td>\n",
       "      <td>1.626953</td>\n",
       "      <td>1.629883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60980 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        F1        F2        F3        F4  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  0.679688  0.676758  0.676758  0.682129   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  0.353516  0.353516  0.353516  0.353516   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  0.534180  0.534180  0.537109  0.539551   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  1.956055  1.956055  1.953125  1.956055   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  1.131836  1.125977  1.128906  1.131836   \n",
       "...                              ...       ...       ...       ...       ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation  0.581055  0.581055  0.581055  0.586426   \n",
       "30486    FOODS_3_824_WI_3_evaluation  0.024658  0.027405  0.030136  0.032867   \n",
       "30487    FOODS_3_825_WI_3_evaluation  0.632812  0.630371  0.627441  0.627441   \n",
       "30488    FOODS_3_826_WI_3_evaluation  1.084961  1.092773  1.092773  1.095703   \n",
       "30489    FOODS_3_827_WI_3_evaluation  1.723633  1.717773  1.707031  1.707031   \n",
       "\n",
       "             F5        F6        F7        F8        F9       F10       F11  \\\n",
       "0      0.682129  0.687500  0.698730  0.693359  0.693359  0.695801  0.698730   \n",
       "1      0.353516  0.353516  0.353516  0.350586  0.350586  0.350586  0.350586   \n",
       "2      0.537109  0.542480  0.542480  0.542480  0.542480  0.539551  0.539551   \n",
       "3      1.961914  1.961914  1.961914  1.953125  1.953125  1.948242  1.945312   \n",
       "4      1.133789  1.128906  1.128906  1.123047  1.125977  1.128906  1.131836   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.591797  0.591797  0.591797  0.591797  0.597168  0.594727  0.591797   \n",
       "30486  0.032867  0.032867  0.032867  0.032867  0.035614  0.035614  0.041107   \n",
       "30487  0.624512  0.630371  0.627441  0.627441  0.619141  0.619141  0.616211   \n",
       "30488  1.101562  1.104492  1.104492  1.109375  1.112305  1.115234  1.120117   \n",
       "30489  1.707031  1.709961  1.698242  1.673828  1.666016  1.652344  1.655273   \n",
       "\n",
       "            F12       F13       F14       F15       F16       F17       F18  \\\n",
       "0      0.698730  0.704102  0.707031  0.712402  0.717773  0.720703  0.720703   \n",
       "1      0.350586  0.347900  0.347900  0.347900  0.347900  0.347900  0.345215   \n",
       "2      0.545410  0.547852  0.553223  0.553223  0.553223  0.556152  0.556152   \n",
       "3      1.951172  1.948242  1.936523  1.931641  1.931641  1.931641  1.931641   \n",
       "4      1.139648  1.145508  1.153320  1.159180  1.164062  1.169922  1.175781   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.594727  0.597168  0.600098  0.597168  0.597168  0.600098  0.597168   \n",
       "30486  0.041107  0.041107  0.041107  0.041107  0.041107  0.041107  0.041107   \n",
       "30487  0.616211  0.616211  0.613770  0.608398  0.610840  0.619141  0.622070   \n",
       "30488  1.120117  1.125977  1.128906  1.131836  1.131836  1.136719  1.139648   \n",
       "30489  1.657227  1.655273  1.657227  1.657227  1.646484  1.640625  1.640625   \n",
       "\n",
       "            F19       F20       F21       F22       F23       F24       F25  \\\n",
       "0      0.726074  0.736816  0.736816  0.736816  0.736816  0.736816  0.745117   \n",
       "1      0.345215  0.347900  0.353516  0.353516  0.356201  0.347900  0.347900   \n",
       "2      0.556152  0.556152  0.559082  0.559082  0.556152  0.556152  0.561523   \n",
       "3      1.928711  1.925781  1.909180  1.907227  1.901367  1.901367  1.909180   \n",
       "4      1.175781  1.175781  1.175781  1.175781  1.177734  1.177734  1.177734   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30485  0.600098  0.600098  0.608398  0.605469  0.608398  0.610840  0.608398   \n",
       "30486  0.041107  0.041107  0.041107  0.041107  0.041107  0.041107  0.043823   \n",
       "30487  0.616211  0.608398  0.610840  0.613770  0.613770  0.616211  0.610840   \n",
       "30488  1.142578  1.145508  1.147461  1.159180  1.175781  1.175781  1.177734   \n",
       "30489  1.643555  1.643555  1.643555  1.646484  1.638672  1.632812  1.635742   \n",
       "\n",
       "            F26       F27       F28  \n",
       "0      0.753418  0.753418  0.756348  \n",
       "1      0.345215  0.345215  0.345215  \n",
       "2      0.566895  0.566895  0.566895  \n",
       "3      1.907227  1.904297  1.909180  \n",
       "4      1.183594  1.186523  1.186523  \n",
       "...         ...       ...       ...  \n",
       "30485  0.608398  0.610840  0.613770  \n",
       "30486  0.043823  0.046570  0.046570  \n",
       "30487  0.608398  0.602539  0.608398  \n",
       "30488  1.180664  1.183594  1.183594  \n",
       "30489  1.638672  1.626953  1.629883  \n",
       "\n",
       "[60980 rows x 29 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 653.380818,
   "position": {
    "height": "675.375px",
    "left": "734.108px",
    "right": "20px",
    "top": "110px",
    "width": "702.766px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
